---
title: "Knit Test"
author: "Anna Bauer, Grant Cai and Alexis Navarra"
date: "Winter 2022"
output:
  html_document: 
    toc: true
    toc_float: true
    collapsed: false
    theme: lumen
    highlight: pygments
    code_folding: show
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction


We chose to explore Spotify music and the attributes that make a song popular because we all listen to music frequently and have specific genres that we enjoy. Something we’ve always wondered about is why we enjoy a particular song, and as a consequence, how certain songs become more popular over others. Is it the artist singing the song, the fact that we’ve heard it on the radio, the beats per minute, or is it whether the lyrics are explicit? To come up with a solution to these questions, we decided to create specific goals - to get to know our data thoroughly, create visualizations of the attributes of our question, run regression models to find which attributes are most relevant, select and fine tune a model, and convey our results in an easily understandable report. This brings us to our research question: can we predict a song’s popularity based on its attributes? 


Many of the current hits are actually created from machine learning algorithms. Whether they are enjoyable to listen to is another topic, however these machine-created hits successfully used algorithms for lyrics, beats, timing, and other factors to make these song popular. We were inspired by this topic in our research question, especially when wondering which factors in these algorithms had the most weight in the song creation. 


```{r, message=FALSE, warning=FALSE}
# list all packages here
library(tidyverse)
library(knitr)
library(lubridate)
library(httpuv)
library(cluster)
library(factoextra)
library(data.table)
library(dplyr)
library(ggplot2)
library(corrplot)

# load music.csv
music <- read.csv("/Users/kimbauer/Desktop/DoubleProjectGroup/131 Project/Spotify Dataset/musicdata.csv")

# music.csv preview
head(music)
```

## Including Plots

You can also embed plots, for example:

```{r}
## manipulation of 'artist' variable

# create a new table that aggregates the variables popularity and artists and finds the mean song popularity for each given artist
artist_pop_table <- aggregate(music$popularity, list(music$artists), FUN = mean)

# merges the table above into our music dataset by artist
full_merge <- merge(x = music, y = artist_pop_table, by.x = c("artists"), by.y = c("Group.1"), all.x = TRUE)

# rename full dataset and x column
full_data <- rename(full_merge, avg_art_pop = x)

# delete previous 'artist' column
delete1 <- c("artists")
full_data <- full_data[!(names(full_data) %in% delete1)]
head(full_data)
```

```{r}
# delete variables that will not be useful in our exploration
delete2 <- c("id", "key", "name", "release_date")
full_data <- full_data[!(names(full_data) %in% delete2)]
```





```{r}
# mutating leftover int variables to num type
full_data$popularity <- as.numeric(full_data$popularity)
full_data$year <- as.numeric(full_data$year)
full_data$duration_ms <- as.numeric(full_data$duration_ms)
```


```{r}
set.seed((123))
# sample data to use for exploratory graphics
sample <- full_data[sample(nrow(full_data), 200), ]

```



```{r}
# mutating dummy variables 
full_data2 = full_data %>% 
  mutate(explicit = as.factor(ifelse(explicit == 0, "Clean", "Explicit"))) %>% 
  mutate(mode = as.factor(ifelse(mode == 0, "Minor", "Major")))
```



```{r}
# split training and testing data
#set seed
set.seed(123)
# Sample 70% of observations as training data 
trainsample = sort(sample(nrow(full_data), nrow(full_data)*.7))
# define dat.train as the 70% of observaions
train = full_data[trainsample,]
# The rest as test data
test = full_data[-trainsample,]
```




```{r, message=FALSE, warning=FALSE}
library(ISLR)
library(tidyverse)
library(class)
library(FNN)

do.chunk <- function(chunkid, folddef, Xdat, Ydat){
  train = (folddef!= chunkid)

  Xtr = Xdat[train,]
  Ytr = Ydat[train]

  Xval = Xdat[!train,]
  Yval = Ydat[!train]
  
  predYtr = knn(train = Xtr, test = Xtr, cl = Ytr)
  predYvl = knn(train = Xtr, test = Xval, cl = Ytr)

  data.frame(fold = chunkid,
             train.error = mean(predYtr != Ytr),
             val.error = mean(predYvl != Yval))
}
```


```{r}
nfold = 10
set.seed(100)
folds = cut(1:nrow(train), breaks = nfold, labels = FALSE) %>% sample()
```

may need to get rid of unlist below:
```{r}
error.folds = NULL
# Give possible number of nearest neighbours to be considered
allK = 1:100
# Set seed since do.chunk() contains a random component induced by knn()
set.seed(888)

Ytrainknn <- train$popularity
Xtrainknn <- train %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)

Ytestknn <- test$popularity
Xtestknn <- test %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)

#below code may not be needed anymore
#Xtrainknn = unlist(Xtrainknn) # unlist to convert from list to vector
#Ytrainknn = unlist(Ytrainknn)

# Loop through different number of neighbors
for (k in allK){
# Loop through different chunk id 
  for (j in seq(3)){
    tmp = do.chunk(chunkid=j, folddef=folds, Xdat=Xtrainknn,
                   Ydat=Ytrainknn)
    tmp$neighbors = k # Record the last number of neighbor
    error.folds = rbind(error.folds, tmp) # combine results }
  }
}

head(error.folds, 10)
```



```{r, message=FALSE, warning=FALSE}
# Transform the format of error.folds for further convenience
errors = melt(error.folds, id.vars=c('fold', 'neighbors'), value.name='error')
# Choose the number of neighbors which minimizes validation error
val.error.means = errors %>%
# Select all rows of validation errors 
  filter(variable=='val.error') %>%
# Group the selected data frame by neighbors 
  group_by(neighbors, variable) %>%
# Calculate CV error rate for each k 
  summarise_each(funs(mean), error) %>%
# Remove existing group
  ungroup() %>%
  filter(error==min(error))
# Best number of neighbors
# if there is a tie, pick larger number of neighbors for simpler model 
numneighbor = max(val.error.means$neighbors)
numneighbor
```


```{r}
# train knn regressor and make predictions on training set using k=20
pred.Ytrainknn = knn.reg(train = Xtrainknn, test = Xtrainknn, y = Ytrainknn, k = numneighbor)
# head(pred.Ytrainknn)
# get training MSE
mean((pred.Ytrainknn$pred - Ytrainknn)^2)

# now make predictions on test set (just to have predict code)
Ytrain <- as.data.frame(Ytrainknn)
pred.Ytestknn = knn.reg(train = Xtrainknn, test = Xtestknn, y = Ytrainknn, k = numneighbor)
head((pred.Ytestknn$pred - Ytestknn)^2)
mean((pred.Ytestknn$pred - Ytestknn)^2)
```

new code for predicting and a plot:
```{r}
# predict code:
library(caret)
pred_knn = predict(pred.Ytrainknn, data.frame(Xtestknn))
```

```{r}
x = 1:length(Ytestknn)

plot(x, Ytestknn, col = "red", type = "l", lwd=2,
     main = "k-NN song popularity prediction")
lines(x, pred_knn, col = "blue", lwd=2)
legend("topright",  legend = c("actual popularity", "predicted popilarity"), 
       fill = c("red", "blue"), col = 2:3,  adj = c(0, 0.6))
grid()
```
end new code



here is snother graph i made for knn:
```{r}
data.plot.k50 <- data.frame(song_popularity = test$popularity,
                           artist_popularity = test$avg_art_pop,
                           Pred = pred.Ytestknn$pred)



data.plot.k50 %>% ggplot(aes(x=artist_popularity, y=song_popularity)) +
  geom_point() +
  geom_line(aes(x = artist_popularity, y=Pred, color="red")) +
  theme(legend.position = "none") +
  ggtitle("k = 50")
```
end graph















