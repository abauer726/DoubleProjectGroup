---
title: "131projectfile"
output: html_document
---

```{r, echo=FALSE}
library(spotifyr)
library(tidyverse)
library(tidymodels)
library(knitr)
library(lubridate)
library(httpuv)
library(cluster)
library(factoextra)
library(data.table)
library(dplyr)
library(ggplot2)
library("ggpubr")
theme_set(
  theme_bw() +
    theme(legend.position = "top")
  )
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
music <- read.csv("/Users/lexnavarra/Desktop/DoubleProjectGroup/131 Project/Spotify Dataset/musicdata.csv")

View(music)


# New research question: can we predict if a song will be a hit based on its attributes? Response variable: song.hotttnesss 
# anna path name: /Users/kimbauer/Desktop/DoubleProjectGroup/131 Project/Spotify Dataset/data_w_genres.csv
```

```{r}
# randomly subset syntax, with 100 points (couldnt run this due to the file not loading)

# anna path name music: /Users/kimbauer/Desktop/DoubleProjectGroup/131 Project/Spotify Dataset/musicdata.csv
# anna path name music w genre: /Users/kimbauer/Desktop/DoubleProjectGroup/131 Project/Spotify Dataset/data_w_genres.csv


# grant path name music: /Users/Grant/DoubleProjectGroup/131 Project/Spotify Dataset/musicdata.csv
# grant path name music w genre: /Users/Grant/DoubleProjectGroup/131 Project/Spotify Dataset/data_w_genres.csv

# lex path name music: /Users/lexnavarra/Desktop/DoubleProjectGroup/131 Project/Spotify Dataset/musicdata.csv
# lex path name music w genre: /Users/lexnavarra/Desktop/DoubleProjectGroup/131 Project/Spotify Dataset/data_w_genres.csv
```


```{r}
# creating a new numeric variable for specific artist average popularity
artist_pop_table <- aggregate(music$popularity, list(music$artists), FUN = mean)

full_merge <- merge(x = music, y = artist_pop_table, by.x = c("artists"), by.y = c("Group.1"), all.x = TRUE)

full_data <- rename(full_merge, avg_art_pop = x)

View(full_data)
```

```{r}
set.seed((666))
# sample data to use for exploratory graphics
sample <- full_data[sample(nrow(full_data), 200), ]
View(sample)
```

```{r pressure, echo=T}
# some EDA plots to decide which predictor to add into our recipe
valence <- ggplot(sample, aes(x=valence, y=popularity)) + geom_point()
valence

year <- ggplot(sample, aes(x=year, y = popularity)) + geom_point()
year

acousticness <- ggplot(sample, aes(x=acousticness, y=popularity)) + geom_point()
acousticness

danceability <- ggplot(sample, aes(x=danceability, y=popularity)) + geom_point()
danceability

duration <- ggplot(sample, aes(x=duration_ms, y=popularity)) + geom_point()
duration

energy <- ggplot(sample, aes(x=energy, y=popularity)) + geom_point()
energy

explicit <- ggplot(sample, aes(x=explicit)) + geom_bar() + scale_x_continuous(breaks = c(0,1), labels=c("0" = "Clean", "1" = "Explicit"))
explicit

instrumentalness <- ggplot(sample, aes(x=instrumentalness, y=popularity)) + geom_point()
instrumentalness

key <- ggplot(sample, aes(x=key)) + geom_bar() + scale_x_continuous(breaks=0:11,
              labels=c("C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B"))
key

liveness <- ggplot(sample, aes(x=liveness, y=popularity)) + geom_point()
liveness

loudness <- ggplot(sample, aes(x=loudness, y=popularity)) + geom_point()
loudness

mode <- ggplot(sample, aes(x=mode)) + geom_bar() + scale_x_continuous(breaks=0:1,
              labels=c("Minor", "Major"))
mode

speechiness <- ggplot(sample, aes(x=speechiness, y=popularity)) + geom_point()
speechiness

tempo <- ggplot(sample, aes(x=tempo, y=popularity)) + geom_point()
tempo

avg_art_pop <- ggplot(sample, aes(x=avg_art_pop, y=popularity)) + geom_point()
avg_art_pop

# columns not included = artists, id, name, release_date
```

```{r}
figure <- ggarrange(tempo, speechiness, explicit,
                    ncol = 2, nrow = 2)
figure


```

```{r}
# modifying our full data to leave out the variables we don't want
full_data$popularity <- as.numeric(full_data$popularity)
full_data$year <- as.numeric(full_data$year)
full_data$duration_ms <- as.numeric(full_data$duration_ms)


full_data2 = full_data %>% 
  mutate(explicit = as.factor(ifelse(explicit == 0, "Clean", "Explicit"))) %>% 
  mutate(mode = as.factor(ifelse(mode == 0, "Minor", "Major")))

delete <- c("artists", "id", "key", "name", "release_date")
full_data <- full_data[!(names(full_data) %in% delete)]
```

```{r}
# split training and testing data

#set seed
set.seed(123)
# Sample 700 observations as training data 
trainsample = sort(sample(nrow(full_data), nrow(full_data)*.7))
# define dat.train as the 700 observstions 
train = full_data[trainsample,]
# The rest as test data
test = full_data[-trainsample,]
```


```{r}
Ytrain <- train$popularity # from lab 5
Xtrain <- train %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)

Ytest <- test$popularity
Xtest <- test %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)

do.chunk <- function(chunkid, folddef, Xdat, Ydat){
  train = (folddef!= chunkid)
  
  Xtr = Xdat[train,]
  Ytr = Ydat[train]
  
  Xval = Xdat[!train,]
  Yval = Ydat[!train]
  
  predYtr = knn(train = Xtr, test = Xtr, cl = Ytr, ...)
  predYvl = knn(train = Ytr, test = Xval, cl = Ytr, ...)
  
  data.frame(fold = chunkid,
             train.error = mean(predYtr != Ytr),
             val.error = mean(predYval != Yval))
}
```

```{r}
# this is cross validation from the lab - not going to use
nfold = 10 

set.seed(100)

folds = cut(1:nrow(train), breaks = nfold, labels = FALSE) %>% sample()
folds

```

```{r}
# this is the other guys method of getting cross validation
folds2 <- vfold_cv(train, v=10, repeats = 5)
```

```{r}
# make recipe 

recipe <- recipe(
  popularity ~ ., data = train) %>%
  step_dummy(mode, explicit, one_hot = TRUE) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors())

recipe
```

```{r}
# BART Implementation
set.seed (1)
bartfit <- gbart(Xtrain , Ytrain , x.test = Xtest)
```

```{r}
Yhat.bart <- bartfit$yhat.test.mean
mean ((Ytest - Yhat.bart)^2) # test error
```

```{r}
# How many times each variable appeared in the collection of trees
ord <- order(bartfit$varcount.mean , decreasing = T)
bartfit$varcount.mean[ord]
```

```{r}
# This doesn't work
bart_workflow <- workflow() %>% 
  add_model(bartfit) %>% 
  add_recipe(recipe)
```

```{r}
# Test error was quite high for BART, checking if boosting is any better....
set.seed (1)
boost.music <- gbm(popularity ~ ., data = train,
  distribution = "gaussian", n.trees = 1000,
  interaction.depth = 4)
```

```{r}
yhat.boost <- predict(boost.music ,
  newdata = train, n.trees = 1000)
mean ((yhat.boost - Ytest)^2)
```