---
title: "131projectfile"
output: html_document
---

```{r, echo=FALSE}
library(spotifyr)
library(tidyverse)
library(tidymodels)
library(knitr)
library(lubridate)
library(httpuv)
library(cluster)
library(factoextra)
library(data.table)
library(dplyr)
library(ggplot2)
library(BART)
library(gbm)
library(kknn)
library(earth)
library(caret)
library(FNN)
library("ggpubr")
library(corrplot)
theme_set(
  theme_bw() +
    theme(legend.position = "top")
  )
```
## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
music <- read.csv("/Users/kimbauer/Desktop/DoubleProjectGroup/131 Project/Spotify Dataset/musicdata.csv")

```


```{r}
# randomly subset syntax, with 100 points (couldnt run this due to the file not loading)

# anna path name music: /Users/kimbauer/Desktop/DoubleProjectGroup/131 Project/Spotify Dataset/musicdata.csv
# anna path name music w genre: /Users/kimbauer/Desktop/DoubleProjectGroup/131 Project/Spotify Dataset/data_w_genres.csv


# grant path name music: /Users/Grant/DoubleProjectGroup/131 Project/Spotify Dataset/musicdata.csv
# grant path name music w genre: /Users/Grant/DoubleProjectGroup/131 Project/Spotify Dataset/data_w_genres.csv

# lex path name music: /Users/lexnavarra/Desktop/DoubleProjectGroup/131 Project/Spotify Dataset/musicdata.csv
# lex path name music w genre: /Users/lexnavarra/Desktop/DoubleProjectGroup/131 Project/Spotify Dataset/data_w_genres.csv
```


```{r}
# creating a new numeric variable for specific artist average popularity
artist_pop_table <- aggregate(music$popularity, list(music$artists), FUN = mean)

full_merge <- merge(x = music, y = artist_pop_table, by.x = c("artists"), by.y = c("Group.1"), all.x = TRUE)

full_data <- rename(full_merge, avg_art_pop = x)
```

```{r}
set.seed((666))
# sample data to use for exploratory graphics
sample <- full_data[sample(nrow(full_data), 200), ]
```


```{r}
# some EDA plots

valence <- ggplot(sample, aes(x=valence, y=popularity)) + geom_point(color = "seagreen1") + geom_smooth(se = FALSE, color = "black")

year <- ggplot(sample, aes(x=year, y = popularity)) + geom_point(color = "dodgerblue2") + geom_smooth(se = FALSE, color = "black") 

acousticness <- ggplot(sample, aes(x=acousticness, y=popularity)) + geom_point(col = "mediumpurple1") 

danceability <- ggplot(sample, aes(x=danceability, y=popularity)) + geom_point(col = "lightpink") + geom_smooth(se = FALSE, color = "black")

duration <- ggplot(sample, aes(x=duration_ms, y=popularity)) + geom_point(col = "sienna1") 

energy <- ggplot(sample, aes(x=energy, y=popularity)) + geom_point(col = "firebrick3") + geom_smooth(se = FALSE, color = "black")

explicit <- ggplot(sample, aes(x=explicit)) + geom_bar(fill = "darkseagreen") + scale_x_continuous(breaks = c(0,1), labels=c("0" = "Clean", "1" = "Explicit"))

instrumentalness <- ggplot(sample, aes(x=instrumentalness, y=popularity)) + geom_point(col = "goldenrod1") 

# key <- ggplot(sample, aes(x=key)) + geom_bar(fill = "darkslategray3") + scale_x_continuous(breaks=0:11,
#             labels=c("C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", #"B"))
# key
# not included because we are dropping this variable in the final dataset

liveness <- ggplot(sample, aes(x=liveness, y=popularity)) + geom_point(col = "deeppink1") 

loudness <- ggplot(sample, aes(x=loudness, y=popularity)) + geom_point(col = "khaki1") + geom_smooth(se = FALSE, color = "black")

mode <- ggplot(sample, aes(x=mode)) + geom_bar(fill = "plum") + scale_x_continuous(breaks=0:1,
              labels=c("Minor", "Major"))

speechiness <- ggplot(sample, aes(x=speechiness, y=popularity)) + geom_point(col = "green4") + geom_smooth(se = FALSE, color = "black")

tempo <- ggplot(sample, aes(x=tempo, y=popularity)) + geom_point(col = "springgreen") 

avg_art_pop <- ggplot(sample, aes(x=avg_art_pop, y=popularity)) + geom_point(col = "deepskyblue") + geom_smooth(se = FALSE, color = "black")

# variables not included as plots = artists, id, name, release_date
```

```{r, fig.width = 12, fig.height = 17}
# making a figure to display all of our EDA plots 
figure <- ggarrange(valence, year, acousticness, danceability, duration, energy, explicit, instrumentalness, liveness, loudness, mode, speechiness, tempo, avg_art_pop, ncol = 3, nrow = 5)
figure

```

```{r}
# making some of the int variables into numeric
full_data$popularity <- as.numeric(full_data$popularity)
full_data$year <- as.numeric(full_data$year)
full_data$duration_ms <- as.numeric(full_data$duration_ms)

# mutating dummy variables 
full_data2 = full_data %>% 
  mutate(explicit = as.factor(ifelse(explicit == 0, "Clean", "Explicit"))) %>% 
  mutate(mode = as.factor(ifelse(mode == 0, "Minor", "Major")))

# modifying our full data to leave out the variables we don't want
delete <- c("artists", "id", "key", "name", "release_date")
full_data <- full_data[!(names(full_data) %in% delete)]
```

```{r}
# correlation matrix and heatmap
# library(corrplot)
corr_mat <- cor(full_data)
corr_mat[, 12]
corrplot(corr_mat, type = "lower", order = "hclust", tl.col = "black", tl.srt = 45)
```
Our two predictors with the strongest correlation to popularity are avg_art_pop and year. This leads us to believe that they will be influential and important to include in our model building. 

```{r}
# adding a density histogram for our outcome variable popularity
hist <- ggplot(full_data) + 
  ggtitle("Density Histogram for Outcome Varible") + # include title
  geom_histogram(aes(popularity, y = after_stat(density)), binwidth = 5, fill = "cyan", color = "grey")  # add histogram w density
x <- seq(0, 100, length.out = 100)
df <- with(full_data, data.frame(x=x, y = dnorm(x, mean(popularity), sd(popularity))))
hist + geom_line(data = df, aes(x = x, y = y), color = "red")
```


```{r}
# split training and testing data

#set seed
set.seed(123)
# Sample 700 observations as training data 
trainsample = sort(sample(nrow(full_data), nrow(full_data)*.7))
# define dat.train as the 700 observstions 
train = full_data[trainsample,]
# The rest as test data
test = full_data[-trainsample,]
```

test adding kfold:
```{r}
library(ISLR)
library(tidyverse)
library(class)
library(FNN)


do.chunk <- function(chunkid, folddef, Xdat, Ydat){
  train = (folddef!= chunkid)

  Xtr = Xdat[train,]
  Ytr = Ydat[train]

  Xval = Xdat[!train,]
  Yval = Ydat[!train]
  
  predYtr = knn(train = Xtr, test = Xtr, cl = Ytr)
  predYvl = knn(train = Xtr, test = Xval, cl = Ytr)

  data.frame(fold = chunkid,
             train.error = mean(predYtr != Ytr),
             val.error = mean(predYvl != Yval))
}


```


```{r}

# this is cross validation from the lab - not going to use
nfold = 10
set.seed(100)
folds = cut(1:nrow(train), breaks = nfold, labels = FALSE) %>% sample()
folds
```


```{r}
error.folds = NULL
# Give possible number of nearest neighbours to be considered
allK = 1:50
# Set seed since do.chunk() contains a random component induced by knn()
set.seed(888)




Ytrainknn <- train$popularity %>% scale(center = TRUE, scale = TRUE) # from lab 5
Xtrainknn <- train %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)

Ytestknn <- test$popularity %>% scale(center = TRUE, scale = TRUE)
Xtestknn <- test %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)


Xtrainknn = unlist(Xtrainknn)
Ytrainknn = unlist(Ytrainknn)
# Loop through different number of neighbors
for (k in allK){
# Loop through different chunk id 
  for (j in seq(3)){
    tmp = do.chunk(chunkid=j, folddef=folds, Xdat=Xtrainknn,
                   Ydat=Ytrainknn)
    tmp$neighbors = k # Record the last number of neighbor
    error.folds = rbind(error.folds, tmp) # combine results }
  }
}

head(error.folds, 10)


```

```{r}
# Transform the format of error.folds for further convenience
errors = melt(error.folds, id.vars=c('fold', 'neighbors'), value.name='error')
# Choose the number of neighbors which minimizes validation error
val.error.means = errors %>%
# Select all rows of validation errors 
  filter(variable=='val.error') %>%
# Group the selected data frame by neighbors 
  group_by(neighbors, variable) %>%
# Calculate CV error rate for each k 
  summarise_each(funs(mean), error) %>%
# Remove existing group
  ungroup() %>%
  filter(error==min(error))
# Best number of neighbors
# if there is a tie, pick larger number of neighbors for simpler model 
numneighbor = max(val.error.means$neighbors)
numneighbor
```


```{r}
## KNN regression work from lab 2

# train knn regressor and make predictions on training set using k=2
pred.Ytrainknn = knn.reg(train = Xtrainknn, test = Xtrainknn, y = Ytrainknn, k = numneighbor)
head(pred.Ytrainknn)
# get training MSE
mean((pred.Ytrainknn$pred - Ytrainknn)^2)
# need to tune and cross validate

# now make predictions on test set (just to have predict code)
Ytrain <- as.data.frame(Ytrainknn)
pred.Ytestknn = knn.reg(train = Xtrainknn, test = Xtestknn, y = Ytrainknn, k = numneighbor)
head((pred.Ytestknn$pred - Ytestknn)^2)
mean((pred.Ytestknn$pred - Ytestknn)^2)
```
end adding kfold



here I am starting over with the KNN using the LOOCV method:
```{r}
library(ISLR)
library(tidyverse)
library(class)
library(FNN)


Ytrainknn <- train$popularity %>% scale(center = TRUE, scale = TRUE) # from lab 5
Xtrainknn <- train %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)

Ytestknn <- test$popularity %>% scale(center = TRUE, scale = TRUE)
Xtestknn <- test %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)


# X and Y training datasets as matrix
Xtrainknn %>% as.matrix()
Ytrainknn %>% as.matrix()

# Give possible number of nearest neighbours to be considered
allK = 1:50
# Set validation.error (a vector of length 50) to save validation errors in future # where validation.error[i] is the LOOCV validation when i-NN method is considered 
validation.error = rep(NA, 50)
# Set random seed to make the results reproducible
set.seed(66)
# For each number in allK, use LOOCV to find a validation error

for (i in allK){
# Loop through different number of neighbors
# Predict on the left-out validation set 
  pred.Yval = knn.cv(train = Xtrainknn, cl=Ytrainknn, k=i)
  # Combine all validation errors 
  validation.error[i] = mean(pred.Yval!=Ytrainknn)
}
# Validation error for 1-NN, 2-NN, ..., 50-NN
plot(allK, validation.error, type = "l", xlab = "k")
```

```{r}
# find best number of neighbors
numneighbor = max(allK[validation.error == min(validation.error)]) 
numneighbor # 48

# Set random seed to make the results reproducible
set.seed(67) # Best k used
pred.Ytestknn = knn(train=Xtrainknn, test=Xtestknn, cl=Ytrainknn, k=numneighbor) # cl iubstead i=f y

# Confusion matrix
conf.matrix = table(predicted=pred.YTestknn, true=Ytestknn)

# Test error rate 
1 - sum(diag(conf.matrix)/sum(conf.matrix))
```

```{r}
mean((pred.Ytestknn$pred - Ytestknn)^2)
```


```{r}
## trying elastic net code
# load packages
library(dplyr)
library(glmnet)
library(ggplot2)
library(caret)

# creating new train and test sets as matrices (from lab 5)
el_X <- model.matrix(popularity ~ ., full_data)[, -1]
el_Y <- full_data$popularity
set.seed(123)
eltrain = sort(sample(nrow(el_X), nrow(el_X)*.7)) # same as train sample above
eltest = (-eltrain)
x.eltrain = el_X[eltrain,]
y.eltrain = el_Y[eltrain]
x.eltest = el_X[eltest,]
y.eltest = el_Y[eltest]

# model building on training sets
control <- trainControl(method = "repeatedcv", 
                        number = 5, # 5 fold
                        repeats = 5, # 5 repeats
                        search = "random", 
                        verboseIter = TRUE)
# training elastic net regression model
elastic_model <- train(x = x.eltrain, y = y.eltrain,
                       method = "glmnet",
                       preProcess = c("center", "scale"),
                       tuneLength = 25, # tuning the model
                       trControl = control) # our cross-validation defined above
elastic_model 
# RMSE was used to select the optimal model using the smallest value.

# plot
plot(elastic_model, main = "Elastic Net Regression")

# print out values from best tuned model
best_elastic <- elastic_model$results %>%
  filter(alpha == elastic_model$bestTune$alpha, lambda == elastic_model$bestTune$lambda)
best_elastic
```

```{r}
# elastic model prediction on test data (to have prediction code)
elastic.pred <- predict(elastic_model, x.eltest)

# evaluate mse on the test data
elastic_mse <- mean((elastic.pred - y.eltest)^2)
elastic_mse
# lasso and ridge account for multi-collinearity, and elastic net is a combination of the two, so no need to investigate collinearity 
```


disregard - non cross validated bart
```{r}
# BART Implementation
Ytrainbart <- train$popularity %>% scale(center = TRUE, scale = TRUE) # from lab 5
Xtrainbart <- train %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)

Ytestbart <- test$popularity %>% scale(center = TRUE, scale = TRUE)
Xtestbart <- test %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)

set.seed (1)
bartfit <- gbart(Xtrainbart, Ytrainbart, x.test = Xtestbart)
```

```{r}
Yhat.bart <- bartfit$yhat.test.mean
mean((Ytest - Yhat.bart)^2) # test error
```

```{r}
# try a prediction on the test set
bart_pred = predict(bartfit, Xtestbart)
```

```{r}
summary(as.double(bart_pred - bartfit$yhat.test))
```
From the above result, we can see that the mean and median values are near zero, indicating that bart_pred (our prediction on the test set) and bartfit$yhat.test () are practically identical



```{r}
dim(Ytestbart)
dim(bart_pred)
```
end disregard







start cross validation version of bart
```{r}
# BART Implementation
Ytrainbart <- train$popularity %>% scale(center = TRUE, scale = TRUE) # from lab 5
Xtrainbart <- train %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)

Ytestbart <- test$popularity %>% scale(center = TRUE, scale = TRUE)
Xtestbart <- test %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)
```

```{r}
# do chunk k-fold cv with BART
do.chunk.bart <- function(chunkid, folddef, Xdat, Ydat, ...){
  # Get training index
  train = (folddef!=chunkid)
  # Get training set by the above index
  Xtr = Xdat[train,]
  # Get responses in training set
  Ytr = Ydat[train]
  # Get validation set
  Xval = Xdat[!train,]
  # Get responses in validation set
  Yval = Ydat[!train]
  # Predict training labels
  predYtr = gbart(Xtr, Ytr, x.test = Xtr)
  # Predict validation labels
  predYval <<- gbart(Xtr , Ytr , x.test = Xval)
  data.frame(fold = chunkid,
    train.error = mean((Ytrainbart - predYtr$yhat.train.mean)^2), # Training error for each fold
    val.error = mean((Ytestbart - predYval$yhat.test.mean)^2))  # Validation error for each fold
}
```

```{r}
# k-fold CV on BART
nfold = 5
# cut: divides all training observations into 3 intervals;
# labels = FALSE instructs R to use integers to code different intervals
set.seed(3)
folds = cut(1:nrow(train), breaks=nfold, labels=FALSE) %>% sample()
folds

# Set error.folds (a vector) to save validation errors in future
error.folds = NULL
# Set seed since do.chunk() contains a random component induced by knn()
set.seed(888)
# Loop through different chunk id
for (i in seq(5)){
  tmp = do.chunk.bart(chunkid=i, folddef=folds, Xdat=Xtrainbart, Ydat=Ytrainbart)
  error.folds = rbind(error.folds, tmp)
}
head(error.folds)
```

```{r we dont need this right now}
pred.YTest = gbart(Xtrainbart, Ytrainbart, x.test = Xtestbart)
# Test mse
test_mse = mean((Ytestbart - pred.YTest$yhat.test.mean)^2)
test_mse
```


```{r}
Yhat.bart <- bartfit$yhat.test.mean
mean((Ytest - Yhat.bart)^2) # test error
```

```{r}
bart_pred2 = predict(predYval, Xtestbart)
```

```{r}
summary(as.double(bart_pred - bartfit$yhat.test))
```
From the above result, we can see that the mean and median values are near zero, indicating that bart_pred (our prediction on the test set) and bartfit$yhat.test () are practically identical


```{r}
# How many times each variable appeared in the collection of trees
ord <- order(bartfit$varcount.mean , decreasing = T)
bartfit$varcount.mean[ord]
```

end cross validation version of bart 






```{r}

trainmars <- train
# implementation of MARS method
# divide dataset into k pieces
# fit a regression model to each piece
# use k-fold cross validation to choose a value for k
# using train


#create a tuning grid
hyper_grid <- expand.grid(degree = 1:3,
                          nprune = seq(2, 50, length.out = 10) %>%
                          floor())

#make this example reproducible
set.seed(1)

#fit MARS model using k-fold cross-validation
cv_mars <- train(
  x = subset(trainmars, select = -c(popularity)),
  y = train$popularity,
  method = "earth",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = hyper_grid)
```

```{r}
#display model with lowest test RMSE
cv_mars$results %>%
  filter(nprune == cv_mars$bestTune$nprune, degree == cv_mars$bestTune$degree)    

#display test RMSE by terms and degree
ggplot(cv_mars)

```
