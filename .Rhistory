as.data.frame() %>%
mutate( variable = row.names(.) ) %>%
filter( Overall > 0 )
plotmo( car,  all1 = T   )
library(plotmo)
imp = varImp( cv_mars  )
imp = imp$importance %>%
as.data.frame() %>%
mutate( variable = row.names(.) ) %>%
filter( Overall > 0 )
plotmo( cv_mars,  all1 = T   )
library(plotmo)
imp = varImp( cv_mars  )
imp = imp$importance %>%
as.data.frame() %>%
mutate(variable = row.names(.) ) %>%
filter(Overall > 0)
plotmo(cv_mars,  all1 = F)
library(plotmo)
imp = varImp( cv_mars  )
imp = imp$importance %>%
as.data.frame() %>%
mutate(variable = row.names(.) ) %>%
filter(Overall > 0)
plotmo(cv_mars,  all1 = T)
# best tuning parameters
train %>%
unnest(best_tune, .drop = F ) %>%
select(popularity, nprune, degree) %>%
knitr::kable( align = 'lcc')
# best tuning parameters
train %>%
unnest(best_tune, .drop = F ) %>%
select(test, nprune, degree) %>%
knitr::kable( align = 'lcc')
# best tuning parameters
mars.pred %>%
select(test, pred, obs ) %>%
gather(key = 'key', value = 'value', -test ) %>%
ggplot( aes( x = value, fill = key ) ) +
geom_histogram( position = 'identity'
, alpha = 0.5) +
facet_wrap(~test, scales = 'free' )
# best tuning parameters
mars.pred %>%
select(test$popularity, pred, obs ) %>%
gather(key = 'key', value = 'value', -test$popularity ) %>%
ggplot( aes( x = value, fill = key ) ) +
geom_histogram( position = 'identity'
, alpha = 0.5) +
facet_wrap(~test$popularity, scales = 'free' )
# best tuning parameters
mars.pred %>%
select(popularity, pred, obs ) %>%
gather(key = 'key', value = 'value', -popularity ) %>%
ggplot( aes( x = value, fill = key ) ) +
geom_histogram( position = 'identity'
, alpha = 0.5) +
facet_wrap(popularity, scales = 'free' )
# best tuning parameters
cv_mars %>%
select(popularity, pred, obs ) %>%
gather(key = 'key', value = 'value', -popularity ) %>%
ggplot( aes( x = value, fill = key ) ) +
geom_histogram( position = 'identity'
, alpha = 0.5) +
facet_wrap(popularity, scales = 'free' )
# best tuning parameters
cv_mars %>%
select(train$popularity, pred, obs ) %>%
gather(key = 'key', value = 'value', -popularity ) %>%
ggplot( aes( x = value, fill = key ) ) +
geom_histogram( position = 'identity'
, alpha = 0.5) +
facet_wrap(popularity, scales = 'free' )
# best tuning parameters
cv_mars %>%
select(train$popularity, pred, obs ) %>%
gather(key = 'key', value = 'value', -train$popularity ) %>%
ggplot( aes( x = value, fill = key ) ) +
geom_histogram( position = 'identity'
, alpha = 0.5) +
facet_wrap(train$popularity, scales = 'free' )
# best tuning parameters
cv_mars %>%
select(train$popularity, pred, obs ) %>%
gather(key = 'key', value = 'value', -train$popularity ) %>%
ggplot( aes( x = value, fill = key ) ) +
geom_histogram( position = 'identity'
, alpha = 0.5) +
facet_wrap(train$popularity, scales = 'free' )
# best tuning parameters
mars_model %>%
select(train$popularity, pred, obs ) %>%
gather(key = 'key', value = 'value', -train$popularity ) %>%
ggplot( aes( x = value, fill = key ) ) +
geom_histogram( position = 'identity'
, alpha = 0.5) +
facet_wrap(train$popularity, scales = 'free' )
cv_mars
mars_model
mars.pred
# best tuning parameters
df_res = cv_mars %>%
select(popularity, y, obs ) %>%
mutate( resid = pred-obs ) %>%
group_by( data_name ) %>%
nest() %>%
mutate( p = pmap( list( df = data, title = data_name),  oetteR::f_plot_pretty_points, col_x = 'obs', col_y = 'resid' )
, p = map(p, function(p) p = p + geom_hline( yintercept = 0, color = 'black', size = 1) ))
# best tuning parameters
df_res = mars.pred %>%
select(popularity, y, obs ) %>%
mutate( resid = pred-obs ) %>%
group_by( data_name ) %>%
nest() %>%
mutate( p = pmap( list( df = data, title = data_name),  oetteR::f_plot_pretty_points, col_x = 'obs', col_y = 'resid' )
, p = map(p, function(p) p = p + geom_hline( yintercept = 0, color = 'black', size = 1) ))
mars.pred
mars.pred
df_mars_visual <- data.frame(mars.pred, test$popularity)
df_mars_visual <- data.frame(mars.pred, test$popularity)
df_mars_visual <- data.frame(mars.pred, test$popularity)
df_mars_visual
resid <- test$popularity - mars.pred
df_mars_visual <- data.frame(mars.pred, test$popularity)
df_mars_visual
resid <- test$popularity - mars.pred
df_mars_visual <- data.frame(test$popularity, mars.pred, resid)
df_mars_visual
plot(df_mars_visual$y.1)
sample_mars <- df_mars_visual[sample(nrow(df_mars_visual), 200), ]
plot(sample_mars$y.1)
sample_mars <- df_mars_visual[sample(nrow(df_mars_visual), 200), ]
plot(sample_mars$y.1)
abline(0,0)
sample_mars <- df_mars_visual[sample(nrow(df_mars_visual), 200), ]
plot(sample_mars$y.1, title = 'MARS residuals')
abline(0,0)
sample_mars <- df_mars_visual[sample(nrow(df_mars_visual), 200), ]
plot(sample_mars$y.1, ylab = 'MARS residuals')
abline(0,0)
sample_mars <- df_mars_visual[sample(nrow(df_mars_visual), 200), ]
plot(sample_mars$y.1, ylab = 'MARS residuals', col = 'blue')
abline(0,0)
sample_mars <- df_mars_visual[sample(nrow(df_mars_visual), 200), ]
plot(sample_mars$y.1, ylab = 'MARS residuals', col = 'blue')
abline(0,0, col = 'red')
sample_mars <- df_mars_visual[sample(nrow(df_mars_visual), 200), ]
plot(sample_mars$y.1, ylab = 'MARS residuals', col = 'blue') +
abline(0,0, col = 'red')
sample_mars <- df_mars_visual[sample(nrow(df_mars_visual), 200), ]
plot(sample_mars$y.1, ylab = 'MARS residuals', col = 'blue', cex = 0.3) +
abline(0,0, col = 'red')
sample_mars <- df_mars_visual[sample(nrow(df_mars_visual), 200), ]
plot(sample_mars$y.1, ylab = 'MARS residuals', col = 'blue', cex = 0.4) +
abline(0,0, col = 'red')
p1 <- vip(cv_mars, num_features = 40, geom = "point", value = "gcv") + ggtitle("GCV")
install.packages('vip')
library(vip)
p1 <- vip(cv_mars, num_features = 14, geom = "point", value = "gcv") + ggtitle("GCV")
library(vip)
p1 <- vip(cv_mars, num_features = 14, geom = "point", value = "gcv") + ggtitle("GCV")
library(vip)
p1 <- vip(cv_mars, num_features = 14, geom = "point", value = "gcv") + ggtitle("GCV")
p1
library(vip)
p1 <- vip(cv_mars, num_features = 16, geom = "point", value = "gcv") + ggtitle("GCV")
p1
library(vip)
p1 <- vip(cv_mars, num_features = 40, geom = "point", value = "gcv") + ggtitle("GCV")
p1
library(vip)
p1 <- vip(cv_mars, num_features = 5, geom = "point", value = "gcv") + ggtitle("GCV")
varImpPlot(cv_mars, sort=T, main="Variable Importance for cv_mars", n.var=5)
library(vip)
p1 <- vip(cv_mars, num_features = 5, geom = "point", value = "gcv") + ggtitle("GCV")
library(gbm)
varImpPlot(cv_mars, sort=T, main="Variable Importance for cv_mars", n.var=5)
library(vip)
p1 <- vip(cv_mars, num_features = 5, geom = "point", value = "gcv") + ggtitle("GCV")
library(ROCR)
varImpPlot(cv_mars, sort=T, main="Variable Importance for cv_mars", n.var=5)
library(vip)
p1 <- vip(cv_mars, num_features = 5, geom = "point", value = "gcv") + ggtitle("GCV")
p1
conf_mat_boost = table(pred = mars.pred, truth = test)
mars.pred
conf_mat_boost = table(pred = mars.pred, truth = test$popularity)
conf_mat_boost
knitr::opts_chunk$set(echo = TRUE)
# list all packages here
library(tidyverse)
library(knitr)
library(lubridate)
library(httpuv)
library(cluster)
library(factoextra)
library(data.table)
library(dplyr)
library(ggplot2)
library(corrplot)
# load music.csv
music <- read.csv("/Users/kimbauer/Desktop/DoubleProjectGroup/131 Project/Spotify Dataset/musicdata.csv")
# list all packages here
library(tidyverse)
library(knitr)
library(lubridate)
library(httpuv)
library(cluster)
library(factoextra)
library(data.table)
library(dplyr)
library(ggplot2)
library(corrplot)
# load music.csv
music <- read.csv("/Users/Grant/DoubleProjectGroup/131 Project/Spotify Dataset/musicdata.csv")
# music.csv preview
head(music)
# create a new table that aggregates the variables popularity and artists and finds the mean song popularity for each given artist
artist_pop_table <- aggregate(music$popularity, list(music$artists), FUN = mean)
# merge the table above into our music dataset by artist
full_merge <- merge(x = music, y = artist_pop_table, by.x = c("artists"), by.y = c("Group.1"), all.x = TRUE)
# rename full dataset and x column
full_data <- rename(full_merge, avg_art_pop = x)
# delete original 'artist' column
delete1 <- c("artists")
full_data <- full_data[!(names(full_data) %in% delete1)]
head(full_data)
# delete variables that will not be useful in our exploration
delete2 <- c("id", "key", "name", "release_date")
full_data <- full_data[!(names(full_data) %in% delete2)]
# mutating leftover int variables to num type
full_data$popularity <- as.numeric(full_data$popularity)
full_data$year <- as.numeric(full_data$year)
full_data$duration_ms <- as.numeric(full_data$duration_ms)
library(ggpubr)
figure <- ggarrange(valence, year, acousticness, danceability, duration, energy, explicit, instrumentalness, liveness, loudness, mode, speechiness, tempo, avg_art_pop, ncol = 3, nrow = 5)
# mutating dummy variables
full_data2 = full_data %>%
mutate(explicit = as.factor(ifelse(explicit == 0, "Clean", "Explicit"))) %>%
mutate(mode = as.factor(ifelse(mode == 0, "Minor", "Major")))
# split training and testing data
#set seed
set.seed(123)
# Sample 70% of observations as training data
trainsample = sort(sample(nrow(full_data), nrow(full_data)*.7))
# define dat.train as the 70% of observaions
train = full_data[trainsample,]
# The rest as test data
test = full_data[-trainsample,]
library(BART)
# BART Implementation
Ytrainbart <- train$popularity # from lab 4
Xtrainbart <- train %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)
Ytestbart <- test$popularity
Xtestbart <- test %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)
# do chunk k-fold cv with BART
do.chunk.bart <- function(chunkid, folddef, Xdat, Ydat, ...){
# Get training index
train = (folddef!=chunkid)
# Get training set by the above index
Xtr = Xdat[train,]
# Get responses in training set
Ytr = Ydat[train]
# Get validation set
Xval = Xdat[!train,]
# Get responses in validation set
Yval = Ydat[!train]
# Predict training labels
predYtr = gbart(Xtr, Ytr, x.test = Xtr)
# Predict validation labels
predYval <<- gbart(Xtr, Ytr, x.test = Xval)
data.frame(fold = chunkid,
train.error = mean((Ytrainbart - predYtr$yhat.train)^2), # Training error for each fold
val.error = mean((Ytestbart - predYval$yhat.test)^2))  # Validation error for each fold
}
# k-fold CV on BART
nfold = 5
# cut: divides all training observations into 3 intervals;
# labels = FALSE instructs R to use integers to code different intervals
set.seed(3)
folds = cut(1:nrow(train), breaks=nfold, labels=FALSE) %>% sample()
folds
# Set error.folds (a vector) to save validation errors in future
error.folds = NULL
# Set seed since do.chunk() contains a random component induced by knn()
set.seed(888)
# Loop through different chunk id
for (i in seq(5)){
tmp = do.chunk.bart(chunkid=i, folddef=folds, Xdat=Xtrainbart, Ydat=Ytrainbart)
error.folds = rbind(error.folds, tmp)
}
head(error.folds)
knitr::opts_chunk$set(echo = TRUE)
# list all packages here
library(tidyverse)
library(knitr)
library(lubridate)
library(httpuv)
library(cluster)
library(factoextra)
library(data.table)
library(dplyr)
library(ggplot2)
library(corrplot)
# load music.csv
music <- read.csv("/Users/Grant/DoubleProjectGroup/131 Project/Spotify Dataset/musicdata.csv")
# music.csv preview
head(music)
# create a new table that aggregates the variables popularity and artists and finds the mean song popularity for each given artist
artist_pop_table <- aggregate(music$popularity, list(music$artists), FUN = mean)
# merge the table above into our music dataset by artist
full_merge <- merge(x = music, y = artist_pop_table, by.x = c("artists"), by.y = c("Group.1"), all.x = TRUE)
# rename full dataset and x column
full_data <- rename(full_merge, avg_art_pop = x)
# delete original 'artist' column
delete1 <- c("artists")
full_data <- full_data[!(names(full_data) %in% delete1)]
head(full_data)
# delete variables that will not be useful in our exploration
delete2 <- c("id", "key", "name", "release_date")
full_data <- full_data[!(names(full_data) %in% delete2)]
# mutating leftover int variables to num type
full_data$popularity <- as.numeric(full_data$popularity)
full_data$year <- as.numeric(full_data$year)
full_data$duration_ms <- as.numeric(full_data$duration_ms)
# mutating dummy variables
full_data2 = full_data %>%
mutate(explicit = as.factor(ifelse(explicit == 0, "Clean", "Explicit"))) %>%
mutate(mode = as.factor(ifelse(mode == 0, "Minor", "Major")))
# split training and testing data
#set seed
set.seed(123)
# Sample 70% of observations as training data
trainsample = sort(sample(nrow(full_data), nrow(full_data)*.7))
# define dat.train as the 70% of observaions
train = full_data[trainsample,]
# The rest as test data
test = full_data[-trainsample,]
library(ISLR) # load libraries
library(tidyverse)
library(class)
library(FNN)
do.chunk <- function(chunkid, folddef, Xdat, Ydat,...){ # for k-fold cross validation
train = (folddef!= chunkid)
Xtr = Xdat[train,] # split x train
Ytr = Ydat[train] # split y train
Xval = Xdat[!train,] # split x validation
Yval = Ydat[!train] # split y validation
predYtr = knn(train = Xtr, test = Xtr, cl = Ytr,...) # knn prediction y train
predYvl = knn(train = Xtr, test = Xval, cl = Ytr,...) # knn prediction y value
data.frame(fold = chunkid,
train.error = mean(predYtr != Ytr),
val.error = mean(predYvl != Yval)) # load into dataset
}
nfold = 10 # 10 folds
set.seed(100) # for reproducibility
folds = cut(1:nrow(train), breaks = nfold, labels = FALSE) %>% sample() # divide data
error.folds = NULL
# Give possible number of nearest neighbours to be considered
allK = 1:50
# Set seed since do.chunk() contains a random component induced by knn()
set.seed(888)
Ytrainknn <- train$popularity # from lab 4
Xtrainknn <- train %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)
Ytestknn <- test$popularity
Xtestknn <- test %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)
# Loop through different number of neighbors
for (k in allK){
# Loop through different chunk id
for (j in seq(3)){
tmp = do.chunk(chunkid=j, folddef=folds, Xdat=Xtrainknn,
Ydat=Ytrainknn, k=k)
tmp$neighbors = k # Record the last number of neighbor
error.folds = rbind(error.folds, tmp) # combine results }
}
}
error.folds = NULL
# Give possible number of nearest neighbours to be considered
allK = 1:50
# Set seed since do.chunk() contains a random component induced by knn()
set.seed(888)
Ytrainknn <- train$popularity # from lab 4
Xtrainknn <- train %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)
Ytestknn <- test$popularity
Xtestknn <- test %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)
# Loop through different number of neighbors
for (k in allK){
# Loop through different chunk id
for (j in seq(10)){
tmp = do.chunk(chunkid=j, folddef=folds, Xdat=Xtrainknn,
Ydat=Ytrainknn, k=k)
tmp$neighbors = k # Record the last number of neighbor
error.folds = rbind(error.folds, tmp) # combine results }
}
}
# Transform the format of error.folds for further convenience
errors = melt(error.folds, id.vars=c('fold', 'neighbors'), value.name='error')
# Choose the number of neighbors which minimizes validation error
val.error.means = errors %>%
# Select all rows of validation errors
filter(variable=='val.error') %>%
# Group the selected data frame by neighbors
group_by(neighbors, variable) %>%
# Calculate CV error rate for each k
summarise_each(funs(mean), error) %>%
# Remove existing group
ungroup() %>%
filter(error==min(error))
# Best number of neighbors
# if there is a tie, pick larger number of neighbors for simpler model
numneighbor = max(val.error.means$neighbors)
numneighbor
# train knn regressor and make predictions on training set using k=98
pred.Ytrainknn = knn.reg(train = Xtrainknn, test = Xtrainknn, y = Ytrainknn, k = numneighbor)
# head(pred.Ytrainknn)
# get training MSE
mean((pred.Ytrainknn$pred - Ytrainknn)^2)
set.seed(888)
# now make predictions on test set (just to have predict code)
Ytrainknn <- as.data.frame(Ytrainknn)
pred.Ytestknn = knn.reg(train = Xtrainknn, test = Xtestknn, y = Ytrainknn, k = numneighbor)
# train knn regressor and make predictions on training set using k=98
pred.Ytrainknn = knn.reg(train = Xtrainknn, test = Xtrainknn, y = Ytrainknn, k = numneighbor)
# Transform the format of error.folds for further convenience
errors = melt(error.folds, id.vars=c('fold', 'neighbors'), value.name='error')
# Choose the number of neighbors which minimizes validation error
val.error.means = errors %>%
# Select all rows of validation errors
filter(variable=='val.error') %>%
# Group the selected data frame by neighbors
group_by(neighbors, variable) %>%
# Calculate CV error rate for each k
summarise_each(funs(mean), error) %>%
# Remove existing group
ungroup() %>%
filter(error==min(error))
# Best number of neighbors
# if there is a tie, pick larger number of neighbors for simpler model
numneighbor = max(val.error.means$neighbors)
numneighbor
# train knn regressor and make predictions on training set using k=98
pred.Ytrainknn = knn.reg(train = Xtrainknn, test = Xtrainknn, y = Ytrainknn, k = numneighbor)
Ytrainknn <- train$popularity # from lab 4
Xtrainknn <- train %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)
Ytestknn <- test$popularity
Xtestknn <- test %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)
# train knn regressor and make predictions on training set using k=98
pred.Ytrainknn = knn.reg(train = Xtrainknn, test = Xtrainknn, y = Ytrainknn, k = numneighbor)
# head(pred.Ytrainknn)
# get training MSE
mean((pred.Ytrainknn$pred - Ytrainknn)^2)
set.seed(888)
# now make predictions on test set (just to have predict code)
Ytrainknn <- as.data.frame(Ytrainknn)
pred.Ytestknn = knn.reg(train = Xtrainknn, test = Xtestknn, y = Ytrainknn, k = numneighbor)
Ytrainknn <- train$popularity # from lab 4
Xtrainknn <- train %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)
Ytestknn <- test$popularity
Xtestknn <- test %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)
# Transform the format of error.folds for further convenience
errors = melt(error.folds, id.vars=c('fold', 'neighbors'), value.name='error')
# Choose the number of neighbors which minimizes validation error
val.error.means = errors %>%
# Select all rows of validation errors
filter(variable=='val.error') %>%
# Group the selected data frame by neighbors
group_by(neighbors, variable) %>%
# Calculate CV error rate for each k
summarise_each(funs(mean), error) %>%
# Remove existing group
ungroup() %>%
filter(error==min(error))
# Best number of neighbors
# if there is a tie, pick larger number of neighbors for simpler model
numneighbor = max(val.error.means$neighbors)
numneighbor
# train knn regressor and make predictions on training set using k=98
pred.Ytrainknn = knn.reg(train = Xtrainknn, test = Xtrainknn, y = Ytrainknn, k = numneighbor)
# head(pred.Ytrainknn)
# get training MSE
mean((pred.Ytrainknn$pred - Ytrainknn)^2)
set.seed(888)
# now make predictions on test set (just to have predict code)
# Ytrainknn <- as.data.frame(Ytrainknn)
pred.Ytestknn = knn.reg(train = Xtrainknn, test = Xtestknn, y = Ytrainknn, k = numneighbor)
head((pred.Ytestknn$pred - Ytestknn)^2)
mean((pred.Ytestknn$pred - Ytestknn)^2)
# train knn regressor and make predictions on training set using k=98
set.seed(888)
pred.Ytrainknn = knn.reg(train = Xtrainknn, test = Xtrainknn, y = Ytrainknn, k = numneighbor)
# head(pred.Ytrainknn)
# get training MSE
mean((pred.Ytrainknn$pred - Ytrainknn)^2)
set.seed(888)
# now make predictions on test set (just to have predict code)
pred.Ytestknn = knn.reg(train = Xtrainknn, test = Xtestknn, y = Ytrainknn, k = numneighbor)
head((pred.Ytestknn$pred - Ytestknn)^2)
mean((pred.Ytestknn$pred - Ytestknn)^2)
ggplot(errors, aes(x=neighbors, y=error, color=variable))+
geom_line(aes(group=interaction(variable,fold))) +
stat_summary(aes(group=variable), fun.y="mean", geom='line', size=3) +
geom_vline(aes(xintercept=numneighbor), linetype='dashed')
ggplot(errors, aes(x=neighbors, y=error, color=variable))+
geom_line(aes(group=interaction(variable,fold))) +
stat_summary(aes(group=variable), fun.y="mean", geom='line', size=50) +
geom_vline(aes(xintercept=numneighbor), linetype='dashed')
ggplot(errors, aes(x=neighbors, y=error, color=variable))+
geom_line(aes(group=interaction(variable,fold))) +
stat_summary(aes(group=variable), fun.y="mean", geom='line', size=3) +
geom_vline(aes(xintercept=numneighbor), linetype='dashed')
