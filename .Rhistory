library(lubridate)
library(httpuv)
library(cluster)
library(factoextra)
library(data.table)
library(dplyr)
library(ggplot2)
library(BART)
library(gbm)
library(kknn)
library(earth)
library(caret)
library(FNN)
library("ggpubr")
theme_set(
theme_bw() +
theme(legend.position = "top")
)
trainmars <- train
# implementation of MARS method
# divide dataset into k pieces
# fit a regression model to each piece
# use k-fold cross validation to choose a value for k
# using train
#create a tuning grid
hyper_grid <- expand.grid(degree = 1:3,
nprune = seq(2, 50, length.out = 10) %>%
floor())
#make this example reproducible
set.seed(1)
#fit MARS model using k-fold cross-validation
cv_mars <- train(
x = subset(trainmars, select = -c(popularity)),
y = train$popularity,
method = "earth",
metric = "RMSE",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid)
trainmars <- train
# implementation of MARS method
# divide dataset into k pieces
# fit a regression model to each piece
# use k-fold cross validation to choose a value for k
# using train
#create a tuning grid
hyper_grid <- expand.grid(degree = 1:3,
nprune = seq(2, 50, length.out = 10) %>%
floor())
#make this example reproducible
set.seed(1)
#fit MARS model using k-fold cross-validation
cv_mars <- train(
x = subset(trainmars, select = -c(popularity)),
y = train$popularity,
method = "earth",
metric = "RMSE",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid)
music <- read.csv("/Users/kimbauer/Desktop/DoubleProjectGroup/131 Project/Spotify Dataset/musicdata.csv")
View(music)
# New research question: can we predict if a song will be a hit based on its attributes? Response variable: song.hotttnesss
# anna path name: /Users/kimbauer/Desktop/DoubleProjectGroup/131 Project/Spotify Dataset/data_w_genres.csv
# creating a new numeric variable for specific artist average popularity
artist_pop_table <- aggregate(music$popularity, list(music$artists), FUN = mean)
full_merge <- merge(x = music, y = artist_pop_table, by.x = c("artists"), by.y = c("Group.1"), all.x = TRUE)
full_data <- rename(full_merge, avg_art_pop = x)
View(full_data)
set.seed((666))
# sample data to use for exploratory graphics
sample <- full_data[sample(nrow(full_data), 200), ]
View(sample)
# modifying our full data to leave out the variables we don't want
full_data$popularity <- as.numeric(full_data$popularity)
full_data$year <- as.numeric(full_data$year)
full_data$duration_ms <- as.numeric(full_data$duration_ms)
full_data2 = full_data %>%
mutate(explicit = as.factor(ifelse(explicit == 0, "Clean", "Explicit"))) %>%
mutate(mode = as.factor(ifelse(mode == 0, "Minor", "Major")))
delete <- c("artists", "id", "key", "name", "release_date")
full_data <- full_data[!(names(full_data) %in% delete)]
# split training and testing data
#set seed
set.seed(123)
# Sample 700 observations as training data
trainsample = sort(sample(nrow(full_data), nrow(full_data)*.7))
# define dat.train as the 700 observstions
train = full_data[trainsample,]
# The rest as test data
test = full_data[-trainsample,]
trainmars <- train
# implementation of MARS method
# divide dataset into k pieces
# fit a regression model to each piece
# use k-fold cross validation to choose a value for k
# using train
#create a tuning grid
hyper_grid <- expand.grid(degree = 1:3,
nprune = seq(2, 50, length.out = 10) %>%
floor())
#make this example reproducible
set.seed(1)
#fit MARS model using k-fold cross-validation
cv_mars <- train(
x = subset(trainmars, select = -c(popularity)),
y = train$popularity,
method = "earth",
metric = "RMSE",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid)
#display model with lowest test RMSE
cv_mars$results %>%
filter(nprune==cv_mars$bestTune$nprune, degree =cv_mars$bestTune$degree)
#display model with lowest test RMSE
cv_mars$results %>%
filter(nprune= cv_mars$bestTune$nprune, degree = cv_mars$bestTune$degree)
library(spotifyr)
library(tidyverse)
library(tidymodels)
library(knitr)
library(lubridate)
library(httpuv)
library(cluster)
library(factoextra)
library(data.table)
library(dplyr)
library(ggplot2)
library(BART)
library(gbm)
library(kknn)
library(earth)
library(caret)
library(FNN)
library("ggpubr")
theme_set(
theme_bw() +
theme(legend.position = "top")
)
music <- read.csv("/Users/kimbauer/Desktop/DoubleProjectGroup/131 Project/Spotify Dataset/musicdata.csv")
View(music)
# New research question: can we predict if a song will be a hit based on its attributes? Response variable: song.hotttnesss
# anna path name: /Users/kimbauer/Desktop/DoubleProjectGroup/131 Project/Spotify Dataset/data_w_genres.csv
# creating a new numeric variable for specific artist average popularity
artist_pop_table <- aggregate(music$popularity, list(music$artists), FUN = mean)
full_merge <- merge(x = music, y = artist_pop_table, by.x = c("artists"), by.y = c("Group.1"), all.x = TRUE)
full_data <- rename(full_merge, avg_art_pop = x)
# creating a new numeric variable for specific artist average popularity
artist_pop_table <- aggregate(music$popularity, list(music$artists), FUN = mean)
full_merge <- merge(x = music, y = artist_pop_table, by.x = c("artists"), by.y = c("Group.1"), all.x = TRUE)
full_data <- rename(full_merge, avg_art_pop = x)
set.seed((666))
# sample data to use for exploratory graphics
sample <- full_data[sample(nrow(full_data), 200), ]
# modifying our full data to leave out the variables we don't want
full_data$popularity <- as.numeric(full_data$popularity)
full_data$year <- as.numeric(full_data$year)
full_data$duration_ms <- as.numeric(full_data$duration_ms)
full_data2 = full_data %>%
mutate(explicit = as.factor(ifelse(explicit == 0, "Clean", "Explicit"))) %>%
mutate(mode = as.factor(ifelse(mode == 0, "Minor", "Major")))
delete <- c("artists", "id", "key", "name", "release_date")
full_data <- full_data[!(names(full_data) %in% delete)]
# split training and testing data
#set seed
set.seed(123)
# Sample 700 observations as training data
trainsample = sort(sample(nrow(full_data), nrow(full_data)*.7))
# define dat.train as the 700 observstions
train = full_data[trainsample,]
# The rest as test data
test = full_data[-trainsample,]
#display model with lowest test RMSE
cv_mars$results %>%
filter(nprune= cv_mars$bestTune$nprune, degree = cv_mars$bestTune$degree)
#display model with lowest test RMSE
cv_mars$results %>%
filter(nprune == cv_mars$bestTune$nprune, degree = cv_mars$bestTune$degree)
#display model with lowest test RMSE
cv_mars$results %>%
filter(nprune == cv_mars$bestTune$nprune, degree = cv_mars$bestTune$degree)
#display model with lowest test RMSE
cv_mars$results %>%
filter(nprune == cv_mars$bestTune$nprune, degree == cv_mars$bestTune$degree)
#display test RMSE by terms and degree
ggplot(cv_mars)
Ytrain <- train$popularity %>% scale(center = TRUE, scale = TRUE)# from lab 5
Ytrain <- train$popularity %>% scale(center = TRUE, scale = TRUE)# from lab 5
Ytrain <- train$popularity %>% scale(center = TRUE, scale = TRUE) # from lab 5
Ytrain <- train$popularity %>% scale(center = TRUE, scale = TRUE) # from lab 5
Ytrain <- train$popularity %>% scale(center = TRUE, scale = TRUE) # from lab 5
Ytrain <- train$popularity %>% scale(center = TRUE, scale = TRUE) # from lab 5
library(spotifyr)
library(tidyverse)
library(tidymodels)
library(knitr)
library(lubridate)
library(httpuv)
library(cluster)
library(factoextra)
library(data.table)
library(dplyr)
library(ggplot2)
library(BART)
library(gbm)
library(kknn)
library(earth)
library(caret)
library(FNN)
library("ggpubr")
theme_set(
theme_bw() +
theme(legend.position = "top")
)
music <- read.csv("/Users/kimbauer/Desktop/DoubleProjectGroup/131 Project/Spotify Dataset/musicdata.csv")
# New research question: can we predict if a song will be a hit based on its attributes? Response variable: song.hotttnesss
# anna path name: /Users/kimbauer/Desktop/DoubleProjectGroup/131 Project/Spotify Dataset/data_w_genres.csv
# creating a new numeric variable for specific artist average popularity
artist_pop_table <- aggregate(music$popularity, list(music$artists), FUN = mean)
full_merge <- merge(x = music, y = artist_pop_table, by.x = c("artists"), by.y = c("Group.1"), all.x = TRUE)
full_data <- rename(full_merge, avg_art_pop = x)
set.seed((666))
# sample data to use for exploratory graphics
sample <- full_data[sample(nrow(full_data), 200), ]
# making some of the int variables into numeric
full_data$popularity <- as.numeric(full_data$popularity)
full_data$year <- as.numeric(full_data$year)
full_data$duration_ms <- as.numeric(full_data$duration_ms)
# mutating dummy variables
full_data2 = full_data %>%
mutate(explicit = as.factor(ifelse(explicit == 0, "Clean", "Explicit"))) %>%
mutate(mode = as.factor(ifelse(mode == 0, "Minor", "Major")))
# modifying our full data to leave out the variables we don't want
delete <- c("artists", "id", "key", "name", "release_date")
full_data <- full_data[!(names(full_data) %in% delete)]
# correlation matrix and heatmap
library(corrplot)
corr_mat <- cor(full_data)
corr_mat[, 12]
corrplot(corr_mat, type = "lower", order = "hclust", tl.col = "black", tl.srt = 45)
# split training and testing data
#set seed
set.seed(123)
# Sample 700 observations as training data
trainsample = sort(sample(nrow(full_data), nrow(full_data)*.7))
# define dat.train as the 700 observstions
train = full_data[trainsample,]
# The rest as test data
test = full_data[-trainsample,]
Ytrain <- train$popularity %>% scale(center = TRUE, scale = TRUE)# from lab 5
Xtrain <- train %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)
Ytest <- test$popularity %>% scale(center = TRUE, scale = TRUE)
Xtest <- test %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)
# do.chunk <- function(chunkid, folddef, Xdat, Ydat){
#   train = (folddef!= chunkid)
#
#   Xtr = Xdat[train,]
#   Ytr = Ydat[train]
#
#   Xval = Xdat[!train,]
#   Yval = Ydat[!train]
#
#   predYtr = knn(train = Xtr, test = Xtr, cl = Ytr, ...)
#   predYvl = knn(train = Ytr, test = Xval, cl = Ytr, ...)
#
#   data.frame(fold = chunkid,
#              train.error = mean(predYtr != Ytr),
#              val.error = mean(predYval != Yval))
# }
## KNN regression work from lab 2
# load libraries
library(ISLR)
library(tidyverse)
library(class)
library(FNN)
# train knn regressor and make predictions on training set using k=2
pred.Ytrain = knn.reg(train = Xtrain, test = Xtrain, y = Ytrain, k = 3)
head(pred.Ytrain)
# get training MSE
mean((pred.Ytrain$pred - Ytrain)^2)
# need to tune and cross validate
# now make predictions on test set (just to have predict code)
# Ytrain <- as.data.frame(Ytrain)
pred.Ytest = knn.reg(train = Xtrain, test = Xtest, y = Ytrain, k = 3)
conf.test = table(predicted=pred.YTest, true=YTest) # confusion matrixz
Ytrain <- train$popularity %>% scale(center = TRUE, scale = TRUE) # from lab 5
Xtrain <- train %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)
Ytest <- test$popularity %>% scale(center = TRUE, scale = TRUE)
Xtest <- test %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)
# Give possible number of nearest neighbours to be considered
allK = 1:50
# Set validation.error (a vector of length 50) to save validation errors in future # where validation.error[i] is the LOOCV validation when i-NN method is considered
validation.error = rep(NA, 50)
# Set random seed to make the results reproducible
set.seed(66)
# For each number in allK, use LOOCV to find a validation error
for (i in allK){
# Loop through different number of neighbors
# Predict on the left-out validation set
pred.Yval = knn.cv(train=Xtrain, cl=Ytrain, k=i)
# Combine all validation errors
validation.error[i] = mean(pred.Yval!=Ytrain)
}
# Validation error for 1-NN, 2-NN, ..., 50-NN
plot(allK, validation.error, type = "l", xlab = "k")
# find best number of neighbors
numneighbor = max(allK[validation.error == min(validation.error)])
numneighbor
# Set random seed to make the results reproducible
set.seed(67) # Best k used
pred.YTest = knn(train=XTrain, test=XTest, cl=YTrain, k=numneighbor)
# find best number of neighbors
numneighbor = max(allK[validation.error == min(validation.error)])
numneighbor
# Set random seed to make the results reproducible
set.seed(67) # Best k used
pred.YTest = knn(train=Xtrain, test=Xtest, cl=Ytrain, k=numneighbor)
# Confusion matrix
conf.matrix = table(predicted=pred.Ytest, true=Ytest)
library(spotifyr)
library(tidyverse)
library(tidymodels)
library(knitr)
library(lubridate)
library(httpuv)
library(cluster)
library(factoextra)
library(data.table)
library(dplyr)
library(ggplot2)
library(BART)
library(gbm)
library(kknn)
library(earth)
library(caret)
library(FNN)
library("ggpubr")
theme_set(
theme_bw() +
theme(legend.position = "top")
)
music <- read.csv("/Users/kimbauer/Desktop/DoubleProjectGroup/131 Project/Spotify Dataset/musicdata.csv")
# New research question: can we predict if a song will be a hit based on its attributes? Response variable: song.hotttnesss
# anna path name: /Users/kimbauer/Desktop/DoubleProjectGroup/131 Project/Spotify Dataset/data_w_genres.csv
# creating a new numeric variable for specific artist average popularity
artist_pop_table <- aggregate(music$popularity, list(music$artists), FUN = mean)
full_merge <- merge(x = music, y = artist_pop_table, by.x = c("artists"), by.y = c("Group.1"), all.x = TRUE)
full_data <- rename(full_merge, avg_art_pop = x)
set.seed((666))
# sample data to use for exploratory graphics
sample <- full_data[sample(nrow(full_data), 200), ]
set.seed((666))
# sample data to use for exploratory graphics
sample <- full_data[sample(nrow(full_data), 200), ]
# some EDA plots
valence <- ggplot(sample, aes(x=valence, y=popularity)) + geom_point(color = "seagreen1") + geom_smooth(se = FALSE, color = "black")
year <- ggplot(sample, aes(x=year, y = popularity)) + geom_point(color = "dodgerblue2") + geom_smooth(se = FALSE, color = "black")
acousticness <- ggplot(sample, aes(x=acousticness, y=popularity)) + geom_point(col = "mediumpurple1")
danceability <- ggplot(sample, aes(x=danceability, y=popularity)) + geom_point(col = "lightpink") + geom_smooth(se = FALSE, color = "black")
duration <- ggplot(sample, aes(x=duration_ms, y=popularity)) + geom_point(col = "sienna1")
energy <- ggplot(sample, aes(x=energy, y=popularity)) + geom_point(col = "firebrick3") + geom_smooth(se = FALSE, color = "black")
explicit <- ggplot(sample, aes(x=explicit)) + geom_bar(fill = "darkseagreen") + scale_x_continuous(breaks = c(0,1), labels=c("0" = "Clean", "1" = "Explicit"))
instrumentalness <- ggplot(sample, aes(x=instrumentalness, y=popularity)) + geom_point(col = "goldenrod1")
key <- ggplot(sample, aes(x=key)) + geom_bar(fill = "darkslategray3") + scale_x_continuous(breaks=0:11,
labels=c("C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B"))
liveness <- ggplot(sample, aes(x=liveness, y=popularity)) + geom_point(col = "deeppink1")
loudness <- ggplot(sample, aes(x=loudness, y=popularity)) + geom_point(col = "khaki1") + geom_smooth(se = FALSE, color = "black")
mode <- ggplot(sample, aes(x=mode)) + geom_bar(fill = "plum") + scale_x_continuous(breaks=0:1,
labels=c("Minor", "Major"))
speechiness <- ggplot(sample, aes(x=speechiness, y=popularity)) + geom_point(col = "green4") + geom_smooth(se = FALSE, color = "black")
tempo <- ggplot(sample, aes(x=tempo, y=popularity)) + geom_point(col = "springgreen")
avg_art_pop <- ggplot(sample, aes(x=avg_art_pop, y=popularity)) + geom_point(col = "deepskyblue") + geom_smooth(se = FALSE, color = "black")
# variables not included as plots = artists, id, name, release_date
# making some of the int variables into numeric
full_data$popularity <- as.numeric(full_data$popularity)
full_data$year <- as.numeric(full_data$year)
full_data$duration_ms <- as.numeric(full_data$duration_ms)
# mutating dummy variables
full_data2 = full_data %>%
mutate(explicit = as.factor(ifelse(explicit == 0, "Clean", "Explicit"))) %>%
mutate(mode = as.factor(ifelse(mode == 0, "Minor", "Major")))
# modifying our full data to leave out the variables we don't want
delete <- c("artists", "id", "key", "name", "release_date")
full_data <- full_data[!(names(full_data) %in% delete)]
# making a figure to display all of our EDA plots
figure <- ggarrange(valence, year, acousticness, danceability, duration, energy, explicit, instrumentalness, key, liveness, loudness, mode, speechiness, tempo, avg_art_pop, ncol = 3, nrow = 5)
figure
fig2 <- rbind(loudness, mode)
fig2 <- rbind(loudness, mode)
fig2
fig2 <- rbind(loudness, mode, size = "first")
fig2
fig2 <- rbind(loudness, explicit, size = "first")
fig2
loudness + explicit
install.packages("patchwork")
library(patchwork)
loudness + explicit
install.packages("patchwork")
library(patchwork)
valence + year + acousticness
library(patchwork)
valence + year + acousticness
danceability + duration + energy
explicit + instrumentalness +  key
liveness +  loudness + mode
speechiness +  tempo + avg_art_pop
library(patchwork)
valence + year + acousticness
danceability + duration + energy
explicit + instrumentalness +  key
liveness +  loudness + mode
speechiness +  tempo + avg_art_pop
library(patchwork)
valence + year + acousticness
danceability + duration + energy
explicit + instrumentalness +  key
liveness +  loudness + mode
speechiness +  tempo + avg_art_pop
library(patchwork)
valence + year + acousticness
danceability + duration + energy
explicit + instrumentalness +  key
liveness +  loudness + mode
speechiness +  tempo + avg_art_pop
library(patchwork)
valence + year + acousticness
danceability + duration + energy
explicit + instrumentalness +  key
liveness +  loudness + mode
speechiness +  tempo + avg_art_pop
# correlation matrix and heatmap
library(corrplot)
corr_mat <- cor(full_data)
corr_mat[, 12]
corrplot(corr_mat, type = "lower", order = "hclust", tl.col = "black", tl.srt = 45)
# find best number of neighbors
numneighbor = max(allK[validation.error == min(validation.error)])
numneighbor
# Set random seed to make the results reproducible
set.seed(67) # Best k used
pred.YTest = knn(train=Xtrain, test=Xtest, cl=Ytrain, k=numneighbor)
## KNN regression work from lab 2
# load libraries
library(ISLR)
library(tidyverse)
library(class)
library(FNN)
# train knn regressor and make predictions on training set using k=2
pred.Ytrain = knn.reg(train = Xtrain, test = Xtrain, y = Ytrain, k = 3)
head(pred.Ytrain)
# find best number of neighbors
numneighbor = max(allK[validation.error == min(validation.error)])
numneighbor
# Set random seed to make the results reproducible
set.seed(67) # Best k used
pred.YTest = knn(train=Xtrain, test=Xtest, cl=Ytrain, k=numneighbor)
# Confusion matrix
conf.matrix = table(predicted=pred.Ytest, true=Ytest)
# find best number of neighbors
numneighbor = max(allK[validation.error == min(validation.error)])
numneighbor # 48
# Set random seed to make the results reproducible
set.seed(67) # Best k used
pred.YTest <- knn(train=Xtrain, test=Xtest, cl=Ytrain, k=numneighbor)
# Confusion matrix
conf.matrix = table(predicted=pred.Ytest, true=Ytest)
# find best number of neighbors
numneighbor = max(allK[validation.error == min(validation.error)])
numneighbor # 48
# Set random seed to make the results reproducible
set.seed(67) # Best k used
pred.YTest = knn(train=Xtrain, test=Xtest, cl=Ytrain, k=48)
# Confusion matrix
conf.matrix = table(predicted=pred.Ytest, true=Ytest)
# find best number of neighbors
numneighbor = max(allK[validation.error == min(validation.error)])
numneighbor # 48
# Set random seed to make the results reproducible
set.seed(67) # Best k used
pred.YTest = knn.reg(train=Xtrain, test=Xtest, y=Ytrain, k=48) # cl iubstead i=f y
# pred.Ytest = knn.reg(train = Xtrain, test = Xtest, y = Ytrain, k = 3)
# Confusion matrix
conf.matrix = table(predicted=pred.Ytest, true=Ytest)
# find best number of neighbors
numneighbor = max(allK[validation.error == min(validation.error)])
numneighbor # 48
# Set random seed to make the results reproducible
set.seed(67) # Best k used
pred.YTest = knn.reg(train=Xtrain, test=Xtest, cl=Ytrain, k=numneighbor) # cl iubstead i=f y
# find best number of neighbors
numneighbor = max(allK[validation.error == min(validation.error)])
numneighbor # 48
# Set random seed to make the results reproducible
set.seed(67) # Best k used
pred.YTest = knn.reg(train=Xtrain, test=Xtest, cl=Ytrain, k=numneighbor) # cl iubstead i=f y
# find best number of neighbors
numneighbor = max(allK[validation.error == min(validation.error)])
numneighbor # 48
# Set random seed to make the results reproducible
set.seed(67) # Best k used
pred.YTest = knn(train=Xtrain, test=Xtest, cl=Ytrain, k=numneighbor) # cl iubstead i=f y
# find best number of neighbors
numneighbor = max(allK[validation.error == min(validation.error)])
numneighbor # 48
# Set random seed to make the results reproducible
set.seed(67) # Best k used
pred.YTest = knn(train=Xtrain, test=Xtest, cl=Ytrain, k=numneighbor) # cl iubstead i=f y
# pred.Ytest = knn.reg(train = Xtrain, test = Xtest, y = Ytrain, k = 3)
# Confusion matrix
conf.matrix = table(predicted=pred.YTest, true=Ytest)
conf.matrix
# Test error rate
1 - sum(diag(conf.matrix)/sum(conf.matrix))
