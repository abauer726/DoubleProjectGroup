# delete previous 'artist' column
delete1 <- c("artists")
full_data <- full_data[!(names(full_data) %in% delete1)]
head(full_data)
# mutating leftover int variables to num type
full_data$popularity <- as.numeric(full_data$popularity)
full_data$year <- as.numeric(full_data$year)
full_data$duration_ms <- as.numeric(full_data$duration_ms)
set.seed((123))
# sample data to use for exploratory graphics
sample <- full_data[sample(nrow(full_data), 200), ]
valence <- ggplot(sample, aes(x=valence, y=popularity)) + geom_point(color = "seagreen1") + geom_smooth(se = FALSE, color = "black")
year <- ggplot(sample, aes(x=year, y = popularity)) + geom_point(color = "dodgerblue2") + geom_smooth(se = FALSE, color = "black")
acousticness <- ggplot(sample, aes(x=acousticness, y=popularity)) + geom_point(col = "mediumpurple1")
danceability <- ggplot(sample, aes(x=danceability, y=popularity)) + geom_point(col = "lightpink") + geom_smooth(se = FALSE, color = "black")
duration <- ggplot(sample, aes(x=duration_ms, y=popularity)) + geom_point(col = "sienna1")
energy <- ggplot(sample, aes(x=energy, y=popularity)) + geom_point(col = "firebrick3") + geom_smooth(se = FALSE, color = "black")
explicit <- ggplot(sample, aes(x=explicit)) + geom_bar(fill = "darkseagreen") + scale_x_continuous(breaks = c(0,1), labels=c("0" = "Clean", "1" = "Explicit"))
instrumentalness <- ggplot(sample, aes(x=instrumentalness, y=popularity)) + geom_point(col = "goldenrod1")
key <- ggplot(sample, aes(x=key)) + geom_bar(fill = "darkslategray3") + scale_x_continuous(breaks=0:11,
labels=c("C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B"))
liveness <- ggplot(sample, aes(x=liveness, y=popularity)) + geom_point(col = "deeppink1")
loudness <- ggplot(sample, aes(x=loudness, y=popularity)) + geom_point(col = "khaki1") + geom_smooth(se = FALSE, color = "black")
mode <- ggplot(sample, aes(x=mode)) + geom_bar(fill = "plum") + scale_x_continuous(breaks=0:1,
labels=c("Minor", "Major"))
speechiness <- ggplot(sample, aes(x=speechiness, y=popularity)) + geom_point(col = "green4")
tempo <- ggplot(sample, aes(x=tempo, y=popularity)) + geom_point(col = "springgreen")
avg_art_pop <- ggplot(sample, aes(x=avg_art_pop, y=popularity)) + geom_point(col = "deepskyblue") + geom_smooth(se = FALSE, color = "black")
library(ggpubr)
figure <- ggarrange(valence, year, acousticness, danceability, duration, energy, explicit, instrumentalness, liveness, loudness, mode, speechiness, tempo, avg_art_pop, ncol = 3, nrow = 5)
figure
# making some of the int variables into numeric
full_data$popularity <- as.numeric(full_data$popularity)
full_data$year <- as.numeric(full_data$year)
full_data$duration_ms <- as.numeric(full_data$duration_ms)
# mutating dummy variables
full_data2 = full_data %>%
mutate(explicit = as.factor(ifelse(explicit == 0, "Clean", "Explicit"))) %>%
mutate(mode = as.factor(ifelse(mode == 0, "Minor", "Major")))
# modifying our full data to leave out the variables we don't want
delete <- c("artists", "id", "key", "name", "release_date")
full_data <- full_data[!(names(full_data) %in% delete)]
# correlation matrix and heatmap
library(corrplot)
corr_mat <- cor(full_data)
corr_mat[, 12]
corrplot(corr_mat, type = "lower", order = "hclust", tl.col = "black", tl.srt = 45)
# adding a density histogram for our outcome variable popularity
hist <- ggplot(full_data) +
ggtitle("Density Histogram for Outcome Varible") + # include title
geom_histogram(aes(popularity, y = after_stat(density)), binwidth = 5, fill = "cyan", color = "grey")  # add histogram w density
x <- seq(0, 100, length.out = 100)
df <- with(full_data, data.frame(x=x, y = dnorm(x, mean(popularity), sd(popularity))))
hist + geom_line(data = df, aes(x = x, y = y), color = "red")
# split training and testing data
#set seed
set.seed(123)
# Sample 70% of observations as training data
trainsample = sort(sample(nrow(full_data), nrow(full_data)*.7))
# define dat.train as the 70% of observaions
train = full_data[trainsample,]
# The rest as test data
test = full_data[-trainsample,]
install.packages('rmdformats')
library(rmdformats)
Y <- full_data$popularity
bartdf <- subset(train, select = -popularity)
Y <- full_data$popularity
bartdf <- subset(train, select = -popularity)
Y
bartdf
Y <- full_data$popularity
bartdf <- subset(train, select = -popularity)
type(Y)
Y <- full_data$popularity
bartdfX <- subset(train, select = -popularity)
Y <- full_data$popularity
bartdfX <- subset(train, select = -popularity)
##build BART regression model
bart_machine_cv = bartMachineCV(X, y)
install.packages("bartMachine")
##build BART regression model
bart_machine_cv = bartMachineCV(bartdfX, Y)
install.packages("bartMachineCV")
library(BartMachine)
install.packages("BartMachine")
install.packages("bartMachine")
library(bartMachine)
install.packages("rJava")
library(bartMachine)
library(bartMachine)
library(bartMachine)
library(rJava)
install.packages("JVM")
library(JVM)
library(rJava)
# library(rJava)
library(bartMachine)
install.packages("rJava")
library(rJava)
# library(rJava)
library(bartMachine)
library(rJava)
# library(rJava)
library(bartMachine)
library(rJava)
sudo R CMD javareconf
install.packages("rJava")
library(rJava)
install.packages("rJava")
# install.packages("rJava")
install.packages("bartMachine")
options(java.parameters = "-Xmx2500m")
library(rJava)
library(bartMachine)
Y <- full_data$popularity
bartdfX <- subset(train, select = -popularity)
##build BART regression model
bart_machine_cv = bartMachineCV(bartdfX, Y)
Y <- train$popularity
bartdfX <- subset(train, select = -popularity)
##build BART regression model
bart_machine_cv = bartMachineCV(bartdfX, Y)
# try a prediction on the test set
bart_pred = predict(bartfit, Xtestbart)
# BART Implementation
Ytrainbart <- train$popularity %>% scale(center = TRUE, scale = TRUE) # from lab 5
library(spotifyr)
library(tidyverse)
library(tidymodels)
library(knitr)
library(lubridate)
library(httpuv)
library(cluster)
library(factoextra)
library(data.table)
library(dplyr)
library(ggplot2)
library(BART)
library(gbm)
library(kknn)
library(earth)
library(caret)
library(FNN)
library("ggpubr")
library(corrplot)
theme_set(
theme_bw() +
theme(legend.position = "top")
)
music <- read.csv("/Users/lexnavarra/Desktop/DoubleProjectGroup/131 Project/Spotify Dataset/musicdata.csv")
music <- read.csv("/Users/kimbauer/Desktop/DoubleProjectGroup/131 Project/Spotify Dataset/musicdata.csv")
# creating a new numeric variable for specific artist average popularity
artist_pop_table <- aggregate(music$popularity, list(music$artists), FUN = mean)
full_merge <- merge(x = music, y = artist_pop_table, by.x = c("artists"), by.y = c("Group.1"), all.x = TRUE)
full_data <- rename(full_merge, avg_art_pop = x)
# making a figure to display all of our EDA plots
figure <- ggarrange(valence, year, acousticness, danceability, duration, energy, explicit, instrumentalness, liveness, loudness, mode, speechiness, tempo, avg_art_pop, ncol = 3, nrow = 5)
figure
# making some of the int variables into numeric
full_data$popularity <- as.numeric(full_data$popularity)
full_data$year <- as.numeric(full_data$year)
full_data$duration_ms <- as.numeric(full_data$duration_ms)
# mutating dummy variables
full_data2 = full_data %>%
mutate(explicit = as.factor(ifelse(explicit == 0, "Clean", "Explicit"))) %>%
mutate(mode = as.factor(ifelse(mode == 0, "Minor", "Major")))
# modifying our full data to leave out the variables we don't want
delete <- c("artists", "id", "key", "name", "release_date")
full_data <- full_data[!(names(full_data) %in% delete)]
# split training and testing data
#set seed
set.seed(123)
# Sample 700 observations as training data
trainsample = sort(sample(nrow(full_data), nrow(full_data)*.7))
# define dat.train as the 700 observstions
train = full_data[trainsample,]
# The rest as test data
test = full_data[-trainsample,]
# BART Implementation
Ytrainbart <- train$popularity %>% scale(center = TRUE, scale = TRUE) # from lab 5
Xtrainbart <- train %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)
Ytestbart <- test$popularity %>% scale(center = TRUE, scale = TRUE)
Xtestbart <- test %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)
set.seed (1)
bartfit <- gbart(Xtrainbart, Ytrainbart, x.test = Xtestbart)
Yhat.bart <- bartfit$yhat.test.mean
mean((Ytest - Yhat.bart)^2) # test error
# try a prediction on the test set
bart_pred = predict(bartfit, Xtestbart)
bart_pred
dim(bart_pred)
# try a prediction on the test set
bart_pred = predict(bartfit, Xtestbart)
dim(Ytestbart)
summary(as.double(bart_pred - Ytestbart))
dim(Ytestbart)
dim(Ytestbart)
dim(bart_pred)
install.packages("rJava")
#install.packages("rJava")
library(rJava)
# library(rJava)
library(bartMachine)
Y <- full_data$popularity
bartdfX <- subset(train, select = -popularity)
# library(rJava)
Y <- full_data$popularity
bartdfX <- subset(train, select = -popularity)
##build BART regression model
bart_machine_cv = bartMachineCV(bartdfX, Y)
# library(rJava)
Y <- train$popularity
bartdfX <- subset(train, select = -popularity)
##build BART regression model
bart_machine_cv = bartMachineCV(bartdfX, Y)
library(spotifyr)
library(tidyverse)
library(tidymodels)
library(knitr)
library(lubridate)
library(httpuv)
library(cluster)
library(factoextra)
library(data.table)
library(dplyr)
library(ggplot2)
library(BART)
library(gbm)
library(kknn)
library(earth)
library(caret)
library(FNN)
library("ggpubr")
library(corrplot)
theme_set(
theme_bw() +
theme(legend.position = "top")
)
music <- read.csv("/Users/kimbauer/Desktop/DoubleProjectGroup/131 Project/Spotify Dataset/musicdata.csv")
# creating a new numeric variable for specific artist average popularity
artist_pop_table <- aggregate(music$popularity, list(music$artists), FUN = mean)
full_merge <- merge(x = music, y = artist_pop_table, by.x = c("artists"), by.y = c("Group.1"), all.x = TRUE)
full_data <- rename(full_merge, avg_art_pop = x)
# making some of the int variables into numeric
full_data$popularity <- as.numeric(full_data$popularity)
full_data$year <- as.numeric(full_data$year)
full_data$duration_ms <- as.numeric(full_data$duration_ms)
# mutating dummy variables
full_data2 = full_data %>%
mutate(explicit = as.factor(ifelse(explicit == 0, "Clean", "Explicit"))) %>%
mutate(mode = as.factor(ifelse(mode == 0, "Minor", "Major")))
# modifying our full data to leave out the variables we don't want
delete <- c("artists", "id", "key", "name", "release_date")
full_data <- full_data[!(names(full_data) %in% delete)]
# split training and testing data
#set seed
set.seed(123)
# Sample 700 observations as training data
trainsample = sort(sample(nrow(full_data), nrow(full_data)*.7))
# define dat.train as the 700 observstions
train = full_data[trainsample,]
# The rest as test data
test = full_data[-trainsample,]
#install.packages("rJava")
library(rJava)
library(bartMachine)
# library(rJava)
Y <- train$popularity
bartdfX <- subset(train, select = -popularity)
##build BART regression model
bart_machine_cv = bartMachineCV(bartdfX, Y)
install.packages("rJava")
options(java.parameters = "-Xmx2500m")
library(rJava)
library(bartMachine)
install.packages("rJava")
##build BART regression model
bart_machine_cv = bartMachineCV(bartdfX, Y)
##build BART regression model
bart_machine_cv = bartMachineCV(bartdfX, Y)
summary(as.double(bart_pred - bartfit$yhat.test))
# try a prediction on the test set
bart_pred = predict(bartfit, Xtestbart)
library(spotifyr)
library(tidyverse)
library(tidymodels)
library(knitr)
library(lubridate)
library(httpuv)
library(cluster)
library(factoextra)
library(data.table)
library(dplyr)
library(ggplot2)
library(BART)
library(gbm)
library(kknn)
library(earth)
library(caret)
library(FNN)
library("ggpubr")
library(corrplot)
theme_set(
theme_bw() +
theme(legend.position = "top")
)
music <- read.csv("/Users/kimbauer/Desktop/DoubleProjectGroup/131 Project/Spotify Dataset/musicdata.csv")
# creating a new numeric variable for specific artist average popularity
artist_pop_table <- aggregate(music$popularity, list(music$artists), FUN = mean)
full_merge <- merge(x = music, y = artist_pop_table, by.x = c("artists"), by.y = c("Group.1"), all.x = TRUE)
full_data <- rename(full_merge, avg_art_pop = x)
# making some of the int variables into numeric
full_data$popularity <- as.numeric(full_data$popularity)
full_data$year <- as.numeric(full_data$year)
full_data$duration_ms <- as.numeric(full_data$duration_ms)
# mutating dummy variables
full_data2 = full_data %>%
mutate(explicit = as.factor(ifelse(explicit == 0, "Clean", "Explicit"))) %>%
mutate(mode = as.factor(ifelse(mode == 0, "Minor", "Major")))
# modifying our full data to leave out the variables we don't want
delete <- c("artists", "id", "key", "name", "release_date")
full_data <- full_data[!(names(full_data) %in% delete)]
# split training and testing data
#set seed
set.seed(123)
# Sample 700 observations as training data
trainsample = sort(sample(nrow(full_data), nrow(full_data)*.7))
# define dat.train as the 700 observstions
train = full_data[trainsample,]
# The rest as test data
test = full_data[-trainsample,]
# BART Implementation
Ytrainbart <- train$popularity %>% scale(center = TRUE, scale = TRUE) # from lab 5
Xtrainbart <- train %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)
Ytestbart <- test$popularity %>% scale(center = TRUE, scale = TRUE)
Xtestbart <- test %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)
set.seed (1)
bartfit <- gbart(Xtrainbart, Ytrainbart, x.test = Xtestbart)
Yhat.bart <- bartfit$yhat.test.mean
mean((Ytest - Yhat.bart)^2) # test error
# try a prediction on the test set
bart_pred = predict(bartfit, Xtestbart)
summary(as.double(bart_pred - bartfit$yhat.test))
bart_pred2 = predict(predYval, Xtestbart)
# do chunk k-fold cv with BART
do.chunk.bart <- function(chunkid, folddef, Xdat, Ydat, ...){
# Get training index
train = (folddef!=chunkid)
# Get training set by the above index
Xtr = Xdat[train,]
# Get responses in training set
Ytr = Ydat[train]
# Get validation set
Xval = Xdat[!train,]
# Get responses in validation set
Yval = Ydat[!train]
# Predict training labels
predYtr = gbart(Xtr, Ytr, x.test = Xtr)
# Predict validation labels
predYval = gbart(Xtr , Ytr , x.test = Xval)
data.frame(fold = chunkid,
train.error = mean((Ytrainbart - predYtr$yhat.train.mean)^2), # Training error for each fold
val.error = mean((Ytestbart - predYval$yhat.test.mean)^2))  # Validation error for each fold
}
# k-fold CV on BART
nfold = 5
# cut: divides all training observations into 3 intervals;
# labels = FALSE instructs R to use integers to code different intervals
set.seed(3)
folds = cut(1:nrow(train), breaks=nfold, labels=FALSE) %>% sample()
folds
# Set error.folds (a vector) to save validation errors in future
error.folds = NULL
# Set seed since do.chunk() contains a random component induced by knn()
set.seed(888)
# Loop through different chunk id
for (i in seq(5)){
tmp = do.chunk.bart(chunkid=i, folddef=folds, Xdat=Xtrainbart, Ydat=Ytrainbart)
error.folds = rbind(error.folds, tmp)
}
head(error.folds)
Yhat.bart <- bartfit$yhat.test.mean
mean((Ytest - Yhat.bart)^2) # test error
bart_pred2 = predict(predYval, Xtestbart)
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(tidyverse)
library(class)
library(FNN)
do.chunk <- function(chunkid, folddef, Xdat, Ydat){
train = (folddef!= chunkid)
Xtr = Xdat[train,]
Ytr = Ydat[train]
Xval = Xdat[!train,]
Yval = Ydat[!train]
predYtr = knn(train = Xtr, test = Xtr, cl = Ytr)
predYvl = knn(train = Xtr, test = Xval, cl = Ytr)
data.frame(fold = chunkid,
train.error = mean(predYtr != Ytr),
val.error = mean(predYvl != Yval))
}
nfold = 10
set.seed(100)
folds = cut(1:nrow(train), breaks = nfold, labels = FALSE) %>% sample()
error.folds = NULL
# Give possible number of nearest neighbours to be considered
allK = 1:50
# Set seed since do.chunk() contains a random component induced by knn()
set.seed(888)
Ytrainknn <- train$popularity %>% scale(center = TRUE, scale = TRUE) # from lab 5
Xtrainknn <- train %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)
Ytestknn <- test$popularity %>% scale(center = TRUE, scale = TRUE)
Xtestknn <- test %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)
Xtrainknn = unlist(Xtrainknn) # unlist to convert from list to vector
Ytrainknn = unlist(Ytrainknn)
# Loop through different number of neighbors
for (k in allK){
# Loop through different chunk id
for (j in seq(3)){
tmp = do.chunk(chunkid=j, folddef=folds, Xdat=Xtrainknn,
Ydat=Ytrainknn)
tmp$neighbors = k # Record the last number of neighbor
error.folds = rbind(error.folds, tmp) # combine results }
}
}
head(error.folds, 10)
# Transform the format of error.folds for further convenience
errors = melt(error.folds, id.vars=c('fold', 'neighbors'), value.name='error')
# list all packages here
library(tidyverse)
library(knitr)
library(lubridate)
library(httpuv)
library(cluster)
library(factoextra)
library(data.table)
library(dplyr)
library(ggplot2)
library(corrplot)
# load music.csv
music <- read.csv("/Users/lexnavarra/Desktop/DoubleProjectGroup/131 Project/Spotify Dataset/musicdata.csv")
# list all packages here
library(tidyverse)
library(knitr)
library(lubridate)
library(httpuv)
library(cluster)
library(factoextra)
library(data.table)
library(dplyr)
library(ggplot2)
library(corrplot)
# load music.csv
music <- read.csv("/Users/Grant/DoubleProjectGroup/131 Project/Spotify Dataset/musicdata.csv")
# music.csv preview
head(music)
## manipulation of 'artist' variable
# create a new table that aggregates the variables popularity and artists and finds the mean song popularity for each given artist
artist_pop_table <- aggregate(music$popularity, list(music$artists), FUN = mean)
# merges the table above into our music dataset by artist
full_merge <- merge(x = music, y = artist_pop_table, by.x = c("artists"), by.y = c("Group.1"), all.x = TRUE)
# rename full dataset and x column
full_data <- rename(full_merge, avg_art_pop = x)
# delete previous 'artist' column
delete1 <- c("artists")
full_data <- full_data[!(names(full_data) %in% delete1)]
head(full_data)
# delete variables that will not be useful in our exploration
delete2 <- c("id", "key", "name", "release_date")
full_data <- full_data[!(names(full_data) %in% delete2)]
# mutating leftover int variables to num type
full_data$popularity <- as.numeric(full_data$popularity)
full_data$year <- as.numeric(full_data$year)
full_data$duration_ms <- as.numeric(full_data$duration_ms)
# Transform the format of error.folds for further convenience
errors = melt(error.folds, id.vars=c('fold', 'neighbors'), value.name='error')
# Choose the number of neighbors which minimizes validation error
val.error.means = errors %>%
# Select all rows of validation errors
filter(variable=='val.error') %>%
# Group the selected data frame by neighbors
group_by(neighbors, variable) %>%
# Calculate CV error rate for each k
summarise_each(funs(mean), error) %>%
# Remove existing group
ungroup() %>%
filter(error==min(error))
# Best number of neighbors
# if there is a tie, pick larger number of neighbors for simpler model
numneighbor = max(val.error.means$neighbors)
numneighbor
# train knn regressor and make predictions on training set using k=20
pred.Ytrainknn = knn.reg(train = Xtrainknn, test = Xtrainknn, y = Ytrainknn, k = numneighbor)
# head(pred.Ytrainknn)
# get training MSE
mean((pred.Ytrainknn$pred - Ytrainknn)^2)
# now make predictions on test set (just to have predict code)
Ytrain <- as.data.frame(Ytrainknn)
pred.Ytestknn = knn.reg(train = Xtrainknn, test = Xtestknn, y = Ytrainknn, k = numneighbor)
head((pred.Ytestknn$pred - Ytestknn)^2)
mean((pred.Ytestknn$pred - Ytestknn)^2)
View(Xtestknn)
res <- predict(pred.Ytrainknn, Xtestknn, "avg_art_pop")
plot(Ytestknn, pred.Ytrainknn$pred, xlab="y", ylab=expression(hat(y)))
plot(Ytestknn, pred.Ytestknn$pred, xlab="y", ylab=expression(hat(y)))
# plot followed from https://quantdev.ssri.psu.edu/sites/qdev/files/kNN_tutorial.html
plot(Ytestknn, pred.Ytestknn$pred, xlab="y", ylab=expression(hat(y)))
# plot followed from https://quantdev.ssri.psu.edu/sites/qdev/files/kNN_tutorial.html
plot(Ytestknn, pred.Ytestknn$pred, xlab="y", ylab=expression(hat(y)))
autoplot(pred.Ytestknn, metric = "rmse")
plot(pred.Ytestknn, metric = "rmse")
