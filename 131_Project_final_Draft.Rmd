---
title: "131 Project Final Draft"
author: "Anna Bauer, Grant Cai, Alexis Navarra"
date: "3/1/2022"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction


For this machine learning project, we chose to explore Spotify music and what makes a song popular because we all listen to music frequently and have specific genres that we enjoy. Something we’ve always wondered about is why we enjoy a particular song, and as a consequence, how certain songs become more popular over others. Is it the artist singing the song, the fact that we’ve heard it on the radio, the beats per minute, or is it whether the lyrics are explicit? To come up with a solution to these questions, our goal is to get to know our data thoroughly, create visualizations of the attributes of our question, run regression models to find which attributes are most relevant, select and fine tune a model, and convey our results in an easily understandable report. This brings us to our research question: can we predict a song’s popularity based on its attributes? 



## Loading Data and Packages


The first step of our machine learning project is simple: find a data set. Only it was not quite as simple as we anticipated. We went through a multitude of datasets to find a dataset that did not face us with problems such as too many missing values, too many observations, too little observations, and more. This process took much longer than we expected, as it comprised of downloading multiple files and attempting to upload them into git, only to discover that they didn't satisfy all of our data needs to explore our research question.

Finally, we landed on a dataset from a Kaggle project called “Music Recommendation System using Spotify Dataset”, for which a codebook is attached in our zip files.  Below, we provide the list of general packages we used for this project (model-specific packages will be included in the model-building section), along with a preview of our dataset. 


```{r cars}
# list all packages here
library(tidyverse)
library(knitr)
library(lubridate)
library(httpuv)
library(cluster)
library(factoextra)
library(data.table)
library(dplyr)
library(ggplot2)
library(corrplot)

# load music.csv
music <- read.csv("/Users/kimbauer/Desktop/DoubleProjectGroup/131 Project/Spotify Dataset/musicdata.csv")

# music.csv preview
head(music)
```



## Data Pre-processing


In our initial look at this dataset, we wanted to identify any concerns early on so that we could either address them in our data pre-processing or look for a new dataset if the concerns were too large to overcome. The first problem in our data we looked for was missing values. After running a summary of the dataset, we found that there were no NA values, which we took as a good sign. We did notice that some of the 'release_date' values only contained the release year and not the exact release data of the song, but we figured this would not be a problem for our exploration. Additionally, we were thrown off by the number of zero values we saw in our initial look at the data, specifically in the columns 'explicit' and 'mode', but after further examination, we realized that they were coded dummy variables, so this was not a problem for our exploration. We also determined that we did not want to drop either of these variables from our data set, as we felt that if a song is explicit or not could be influential in how popular it becomes, and we felt that if a song was in a minor key or a major key could also be influential in how popular it becomes.


Thus, we begin our data pre-processing by mutating our dummy variables `explicit` and `mode` into as.factor variables. If, for a given song, explicit = 0, we label that as "Clean", and if explicit = 1, we label that "Explicit". If, for a given song, mode = 0, we label that as "Minor", and if mode = 1, we label that as "Major". 


```{r}
# mutate our dummy variables into as.factor
# music = music %>% 
#   mutate(explicit = as.factor(ifelse(explicit == 0, "Clean", "Explicit"))) %>% 
#   mutate(mode = as.factor(ifelse(mode == 0, "Minor", "Major")))
```


As our research question requires us to take a regression approach, we did have a concern of our data having too many categorical variables to fit into our regressive model with this data. To address this problem, we thought about what variables would be most important to our data exploration. One of the variables that we thought would be most important to include was 'artists', as we know that the artist of a song can have a direct impact on the song's popularity: a more popular artist is more likely to release a song that has higher popularity, and vice versa. With the `artists` column being string-valued categorical observations, and with there being so many different artists in our data, we wanted to find a way to incorporate this variable into our data as a unique but quantitative variable. We decided that this would be the next step of our data mutation


In order to keep the important variable of `artists` but make it quantitative, we decided to take the average song popularity by each artist in the data, and assign that variable of average artist popularity to each artist as a new variable in the dataset. Thus, our dataset would lose the column 'artists' containing each artist's name, and instead assign the variable `avg_art_pop` as the artist's average song popularity. This variable mutation process is outlined in the code chunk below.


```{r}
## manipulation of 'artist' variable

# create a new table that aggregates the variables popularity and artists and finds the mean song popularity for each given artist
artist_pop_table <- aggregate(music$popularity, list(music$artists), FUN = mean)

# merges the table above into our music dataset by artist
full_merge <- merge(x = music, y = artist_pop_table, by.x = c("artists"), by.y = c("Group.1"), all.x = TRUE)

# rename full dataset and x column
full_data <- rename(full_merge, avg_art_pop = x)

# delete previous 'artist' column
delete1 <- c("artists")
full_data <- full_data[!(names(full_data) %in% delete1)]
head(full_data)
```


In addressing the multiple categorical variables we had left in our dataset, we then set out to determine if these variables would be important to our exploration and if they would be feasible to incorporate quantitatively into our data (similar to `artists` above). The qualitative variables that we had left in our dataset (that we decided would be irrational to dummy code) were `id`, `key`, `name`, `release_date`. We discussed each of these variables as a group, and decided that none of them would be very useful to try and fit quantitatively into our final data set, as we don't feel they will logically be important in predicting a given song's average popularity. In addition, analysis of the plots of some of these variables, such as `key`, demonstrate no real key being favored over others (see Exploratory Data Anallysis). Thus, we decided to drop these variables from our dataset, as shown below.


```{r}
# delete variables that will not be useful in our exploration
delete2 <- c("id", "key", "name", "release_date")
full_data <- full_data[!(names(full_data) %in% delete2)]
```


And finally, after making sure that all of our quantitative variables were numeric (shown in code chunk below), our data was ready for some exploratory data analysis. 


```{r}
# mutating leftover int variables to num type
full_data$popularity <- as.numeric(full_data$popularity)
full_data$year <- as.numeric(full_data$year)
full_data$duration_ms <- as.numeric(full_data$duration_ms)
```



## Exploratory Data Analysis

We first set out to explore the relationship between the various predictors and the response variable, popularity. We first subset the data to a more manageable size so that we could discern something from exploratory data analysis:
```{r}
set.seed((123))
# sample data to use for exploratory graphics
sample <- full_data[sample(nrow(full_data), 200), ]
```

COMMENT: TO KEY OR NOT TO KEY, DEBATE LATER IF TIME

```{r, echo = FALSE}
valence <- ggplot(sample, aes(x=valence, y=popularity)) + geom_point(color = "seagreen1") + geom_smooth(se = FALSE, color = "black")

year <- ggplot(sample, aes(x=year, y = popularity)) + geom_point(color = "dodgerblue2") + geom_smooth(se = FALSE, color = "black") 

acousticness <- ggplot(sample, aes(x=acousticness, y=popularity)) + geom_point(col = "mediumpurple1") 

danceability <- ggplot(sample, aes(x=danceability, y=popularity)) + geom_point(col = "lightpink") + geom_smooth(se = FALSE, color = "black")

duration <- ggplot(sample, aes(x=duration_ms, y=popularity)) + geom_point(col = "sienna1") 

energy <- ggplot(sample, aes(x=energy, y=popularity)) + geom_point(col = "firebrick3") + geom_smooth(se = FALSE, color = "black")

explicit <- ggplot(sample, aes(x=explicit)) + geom_bar(fill = "darkseagreen") + scale_x_continuous(breaks = c(0,1), labels=c("0" = "Clean", "1" = "Explicit"))

instrumentalness <- ggplot(sample, aes(x=instrumentalness, y=popularity)) + geom_point(col = "goldenrod1") 

key <- ggplot(sample, aes(x=key)) + geom_bar(fill = "darkslategray3") + scale_x_continuous(breaks=0:11,
            labels=c("C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B"))

liveness <- ggplot(sample, aes(x=liveness, y=popularity)) + geom_point(col = "deeppink1") 

loudness <- ggplot(sample, aes(x=loudness, y=popularity)) + geom_point(col = "khaki1") + geom_smooth(se = FALSE, color = "black")

mode <- ggplot(sample, aes(x=mode)) + geom_bar(fill = "plum") + scale_x_continuous(breaks=0:1,
              labels=c("Minor", "Major"))

speechiness <- ggplot(sample, aes(x=speechiness, y=popularity)) + geom_point(col = "green4")

tempo <- ggplot(sample, aes(x=tempo, y=popularity)) + geom_point(col = "springgreen") 

avg_art_pop <- ggplot(sample, aes(x=avg_art_pop, y=popularity)) + geom_point(col = "deepskyblue") + geom_smooth(se = FALSE, color = "black")
```

After defining our various plots, we accumulate them into a panel for better visibility:
```{r, fig.width = 5, fig.height = 2}
# library(patchwork)
# valence + year + acousticness
# danceability + duration + energy
# explicit + instrumentalness +  key
# liveness +  loudness + mode
# speechiness +  tempo + avg_art_pop
```

```{r, fig.width = 12, fig.height = 17}
library(ggpubr)
figure <- ggarrange(valence, year, acousticness, danceability, duration, energy, explicit, instrumentalness, liveness, loudness, mode, speechiness, tempo, avg_art_pop, ncol = 3, nrow = 5)
figure
```


There are several things that can be extrapolated from the panel of plots: mainly, many of the predictors, such as `valence`, `acousticness`, `danceability`, `duration_ms`, and `tempo` all have fairly weak correlations with `popularity`. We also learn that many songs in the dataset appear to be around 0 for `speechiness`, `liveness`, and `acousticness`. Regarding the categorical variables `explicit` and `mode`, there is a clear skew towards songs that are clean and in a major key.

Looking specifically at the variables that have the strongest relationships with popularity, the two predictors `year` and `avg_art_pop` appear to have the strongest positive correlation with popularity. This makes intuitive sense. 

There are several factors that could explain why year has a positive, near linear, relationship with popularity: for one, the population has increased from the 1960s until present day. With more people comes a more active listening base for artists to reach. In addition, the methods by which we obtain music have changed over the decades, starting off with things like records and record players, CDs, iTunes, and now with music streaming through platforms like Spotify.

It makes intuitive sense why an artist's average popularity would be positively correlated with popularity. An artist with high brand value or recognition, such as Kendrick Lamar, Ariana Grande, or Justin Bieber are more likely to have new releases listened to by significantly more people as opposed to a more obscure or lesser known artist such as Gus Dapperton or Peach Pit. 

This relationship is clearly shown through a correlation matrix and subsequent heatmap:
```{r}
# making some of the int variables into numeric
full_data$popularity <- as.numeric(full_data$popularity)
full_data$year <- as.numeric(full_data$year)
full_data$duration_ms <- as.numeric(full_data$duration_ms)

# mutating dummy variables 
full_data2 = full_data %>% 
  mutate(explicit = as.factor(ifelse(explicit == 0, "Clean", "Explicit"))) %>% 
  mutate(mode = as.factor(ifelse(mode == 0, "Minor", "Major")))

# modifying our full data to leave out the variables we don't want
delete <- c("artists", "id", "key", "name", "release_date")
full_data <- full_data[!(names(full_data) %in% delete)]
```

```{r}
# correlation matrix and heatmap
library(corrplot)
corr_mat <- cor(full_data)
corr_mat[, 12]
corrplot(corr_mat, type = "lower", order = "hclust", tl.col = "black", tl.srt = 45)
```

The heatmap reinforces the notion that the two predictors with the strongest positive association with popularity are `year` and `avg_art_pop`. We thus have reason to believe that they will be influential and important to include in our model building.

Finally, an analysis of the distribution of the response `popularity` allows us to see that it is pretty normally-distributed.

```{r}
# adding a density histogram for our outcome variable popularity
hist <- ggplot(full_data) + 
  ggtitle("Density Histogram for Outcome Varible") + # include title
  geom_histogram(aes(popularity, y = after_stat(density)), binwidth = 5, fill = "cyan", color = "grey")  # add histogram w density
x <- seq(0, 100, length.out = 100)
df <- with(full_data, data.frame(x=x, y = dnorm(x, mean(popularity), sd(popularity))))
hist + geom_line(data = df, aes(x = x, y = y), color = "red")
```


# Model Fitting

Now that we have explored our data visually, we can begin to form our models. We looked the characteristics of our data, such as the number of observations, type of data (qualitative vs quantitative), etcetera, and found that an elastic net model, a Bayesian additive regression trees (BART) model, a multivariate adaptive regression splines (MARS) model, and a k-nearest neighbors (KNN) model would suit our research question and data well. Each of these algorithms fit our regression-based question well. We will explore the reasons why we chose each model later in this section. 

### Splitting our data
Since we have 120,750 observations (songs) in total, we can safely split our dataset into train and test sets. This way we can use the training data to fit our model and save the test data to evaluate how well our model predicts the data (test MSE). We chose to use 70% of the data for training and 30% of the data for testing. To implement this, we set the seed for reproductability, took a random sample of 70% of the dataset, and used that sample to create a training set. The rest of the values were used for the testing set. Below is an illustration of how the elastic net uses both the lasso and ridge techniques. 



```{r}
# split training and testing data
#set seed
set.seed(123)
# Sample 70% of observations as training data 
trainsample = sort(sample(nrow(full_data), nrow(full_data)*.7))
# define dat.train as the 70% of observaions
train = full_data[trainsample,]
# The rest as test data
test = full_data[-trainsample,]
```


## K-NN

The k-nearest neighbors regression algorithm is a non-parametric method that takes observations from different neighbors to approximate the association between the predictors and the outcome (in our case song popularity). KNN is a good fit for our research question because the function can have many function forms other than linear, which is what we are looking for. 

The downside about that KNN algorithm is that it can only be used when the variances of variables are similar. By using this model, we are making the assumption that the variances of each predictor of probability are similar enough. Further, KNN makes it difficult to "predict into the future", which, alongside the curse of dimensionality, makes it not very interpretable. 

For the KNN model, our tuning parameter is k. 

To implement our model, we first load the required packages and use the do.chunk() function for k-fold cross-validation. We use folddef to specify an index to map all observations in the design matrix and all observations into the response into a set of k folds. chunkid is used to set the fold used as the validation set. The do.chunk function as a whole trains the model using all folds but the chunkid fold, and computes the validation error for that last fold. The resulting dataframe consists of all possible values of the folds with their test error rate. 
```{r}
library(ISLR)
library(tidyverse)
library(class)
library(FNN)


do.chunk <- function(chunkid, folddef, Xdat, Ydat){
  train = (folddef!= chunkid)

  Xtr = Xdat[train,]
  Ytr = Ydat[train]

  Xval = Xdat[!train,]
  Yval = Ydat[!train]
  
  predYtr = knn(train = Xtr, test = Xtr, cl = Ytr)
  predYvl = knn(train = Xtr, test = Xval, cl = Ytr)

  data.frame(fold = chunkid,
             train.error = mean(predYtr != Ytr),
             val.error = mean(predYvl != Yval))
}
```


Next, we specify that we would like 10 folds and divide the data into ten intervals. 
```{r}
nfold = 10
set.seed(100)
folds = cut(1:nrow(train), breaks = nfold, labels = FALSE) %>% sample()
```

Next, we decide to have 50 neighbors considered, and loop through all 50 of the neighbor for each different chunk id. Below is a dataframe of the error folds and their respective training error, validation error, and neighbors. 
```{r}
error.folds = NULL
# Give possible number of nearest neighbours to be considered
allK = 1:50
# Set seed since do.chunk() contains a random component induced by knn()
set.seed(888)


Ytrainknn <- train$popularity %>% scale(center = TRUE, scale = TRUE) # from lab 5
Xtrainknn <- train %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)

Ytestknn <- test$popularity %>% scale(center = TRUE, scale = TRUE)
Xtestknn <- test %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)


Xtrainknn = unlist(Xtrainknn) # unlist to convert from list to vector
Ytrainknn = unlist(Ytrainknn)
# Loop through different number of neighbors
for (k in allK){
# Loop through different chunk id 
  for (j in seq(3)){
    tmp = do.chunk(chunkid=j, folddef=folds, Xdat=Xtrainknn,
                   Ydat=Ytrainknn)
    tmp$neighbors = k # Record the last number of neighbor
    error.folds = rbind(error.folds, tmp) # combine results }
  }
}

head(error.folds, 10)
```

Next, we want to decide the optimal k for kNN, using error.folds. We want the type (training or test) of error and the corresponding value for each fold and each neighbor. We can do this using the melt() function.
```{r}
# Transform the format of error.folds for further convenience
errors = melt(error.folds, id.vars=c('fold', 'neighbors'), value.name='error')
# Choose the number of neighbors which minimizes validation error
val.error.means = errors %>%
# Select all rows of validation errors 
  filter(variable=='val.error') %>%
# Group the selected data frame by neighbors 
  group_by(neighbors, variable) %>%
# Calculate CV error rate for each k 
  summarise_each(funs(mean), error) %>%
# Remove existing group
  ungroup() %>%
  filter(error==min(error))
# Best number of neighbors
# if there is a tie, pick larger number of neighbors for simpler model 
numneighbor = max(val.error.means$neighbors)
numneighbor
```
Thus, we found that a [insert here]-NN classifier would be best. Now we can calculate the test MSE using the knn function. 

```{r}
# train knn regressor and make predictions on training set using k=20
pred.Ytrainknn = knn.reg(train = Xtrainknn, test = Xtrainknn, y = Ytrainknn, k = numneighbor)
# head(pred.Ytrainknn)
# get training MSE
mean((pred.Ytrainknn$pred - Ytrainknn)^2)

# now make predictions on test set (just to have predict code)
Ytrain <- as.data.frame(Ytrainknn)
pred.Ytestknn = knn.reg(train = Xtrainknn, test = Xtestknn, y = Ytrainknn, k = numneighbor)
head((pred.Ytestknn$pred - Ytestknn)^2)
mean((pred.Ytestknn$pred - Ytestknn)^2)
```
Thus, we found or training MSE to be 0.3100 and our test MSE to be 0.3490 for this model. This is a considerably low MSE, and the training and test MSEs being so similar leads us to believe that the k-NN model properly prevented overfitting. 




## Elastic Net Model Fitting

Our next model is the elastic net, a combination of the ridge and lasso techniques. The elastic net method is a regularized linear regression that combines L1 and L2 penalty functions from the lasso and ridge techniques. Thus, the elastic net performs variable selection and regularization simultaneously. We found the elastic net to be more ideal than the ridge and lasso techniques on their own because it uses both methods and learns from them.

The idea of the elastic net is to minimize the residual sum of squares and add a penalty term:

$$\sum\limits_{i=1}^n (y_i - \beta_0 - \sum\limits_{j=1}^p \beta_j x_{ij})^2 + \lambda[(1 - \alpha)\ ||\beta||_2^2/2 + \alpha\ ||\beta||_1]$$
where $||\beta||_1$ takes the form of L1 (L1 norm):
$$||\beta||_1 = \sum\limits_{j=1}^p |\beta_j|$$

and $||\beta||_2$ takes the form of L2 (Euclidean norm):
$$||\beta||_2 = \sqrt{\sum\limits_{j=1}^p \beta_j^2}$$
Here is a visual explanation of the elastic net: 
![elastic net implementation.](/Users/kimbauer/Desktop/DoubleProjectGroup/131 Project/elasticnetimage.png)

This model was a good choice for our data because it's a regression model, which fits our mostly quantitative variables, and it improves on the lasso and ridge techniques. Additionally, the lasso and ridge methods account for collinearity, so that eliminates our need to investigate collinearity between predictors. 
```{r}
# load packages
library(dplyr) # data manipulation
library(glmnet) # elastic net 
library(ggplot2) # for visualization
library(caret) # classofication and regression training

# creating new train and test sets as matrices (from lab 5)
el_X <- model.matrix(popularity ~ ., full_data)[, -1]
el_Y <- full_data$popularity
set.seed(123)
eltrain = sort(sample(nrow(el_X), nrow(el_X)*.7)) # same as train sample above
eltest = (-eltrain)
x.eltrain = el_X[eltrain,]
y.eltrain = el_Y[eltrain]
x.eltest = el_X[eltest,]
y.eltest = el_Y[eltest]

# model building on training sets
control <- trainControl(method = "repeatedcv", 
                        number = 5, # 5 fold
                        repeats = 5, # 5 repeats
                        search = "random", 
                        verboseIter = TRUE)
# training elastic net regression model
elastic_model <- train(x = x.eltrain, y = y.eltrain,
                       method = "glmnet",
                       preProcess = c("center", "scale"),
                       tuneLength = 25, # tuning the model
                       trControl = control) # our cross-validation defined above
elastic_model 
# RMSE was used to select the optimal model using the smallest value.

# plot
plot(elastic_model, main = "Elastic Net Regression")

# print out values from best tuned model
best_elastic <- elastic_model$results %>%
  filter(alpha == elastic_model$bestTune$alpha, lambda == elastic_model$bestTune$lambda)
best_elastic
```



Insert inrepretation of this result. The elastic net algorithm does tune and cross validate on its own, so there is no need for us to perform further cross validation or tuning. 


Now that we have our model, we want to make a prediction on our test data. To do this, we use the predict() function to get the test mean square error. Later we will compare the test MSEs of different functions to decide which model is optimal for our data. 
```{r}
# elastic model prediction on test data (to have prediction code)
elastic.pred <- predict(elastic_model, x.eltest)

# evaluate mse on the test data
elastic_mse <- mean((elastic.pred - y.eltest)^2)
elastic_mse
```



## Bayesian Additive Regression Trees (BART)

The BART method uses decision trees for regression, constructing each tree randomly, similar to bagging and random forest models. Each new tree is trying to capture "signal" that is not accounted for by the rest of the trees. In the BART model, K is the number of regression trees and B is the number of iterations that the BART will run through. 

Below is a description of the algorithm we used to construct our BART model. 
![from ISLR textbook](/Users/kimbauer/Desktop/DoubleProjectGroup/131 Project/BARTalgorithm.png)

First, we got our training and testing data and split them into Xtrain, Ytrain, Xtest, and Ytest for the BART model. Then we were able to use the gbart() function to fit a model using BART. 

```{r}
library(BART)

# BART Implementation
Ytrainbart <- train$popularity %>% scale(center = TRUE, scale = TRUE) # from lab 5
Xtrainbart <- train %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)

Ytestbart <- test$popularity %>% scale(center = TRUE, scale = TRUE)
Xtestbart <- test %>% select(-popularity) %>% scale(center = TRUE, scale = TRUE)

set.seed (1)
bartfit <- gbart(Xtrain , Ytrain , x.test = Xtest)
```

Next, we found the test error for BART model:
```{r}
Yhat.bart <- bartfit$yhat.test.mean
mean((Ytest - Yhat.bart)^2) # test error
```

Next, we found that each variable appeared [insert here] times in the collection of trees:
```{r}
# How many times each variable appeared in the collection of trees
ord <- order(bartfit$varcount.mean , decreasing = T)
bartfit$varcount.mean[ord]
```

Finally, we implemented k-fold cross validation with our BART model:
```{r}
# do chunk k-fold cv with BART
do.chunk.bart <- function(chunkid, folddef, Xdat, Ydat, ...){
  # Get training index
  train = (folddef!=chunkid)
  # Get training set by the above index
  Xtr = Xdat[train,]
  # Get responses in training set
  Ytr = Ydat[train]
  # Get validation set
  Xval = Xdat[!train,]
  # Get responses in validation set
  Yval = Ydat[!train]
  # Predict training labels
  predYtr = gbart(Xtr, Ytr, x.test = Xtr)
  # Predict validation labels
  predYval = gbart(Xtr , Ytr , x.test = Xval)
  data.frame(fold = chunkid,
    train.error = mean((Ytrainbart - predYtr$yhat.train.mean)^2), # Training error for each fold
    val.error = mean((Ytestbart - predYval$yhat.test.mean)^2))  # Validation error for each fold
}
```

```{r}
# k-fold CV on BART
nfold = 5
# cut: divides all training observations into 3 intervals;
# labels = FALSE instructs R to use integers to code different intervals
set.seed(3)
folds = cut(1:nrow(train), breaks=nfold, labels=FALSE) %>% sample()
folds

# Set error.folds (a vector) to save validation errors in future
error.folds = NULL
# Set seed since do.chunk() contains a random component induced by knn()
set.seed(888)
# Loop through different chunk id
for (i in seq(5)){
  tmp = do.chunk.bart(chunkid=i, folddef=folds, Xdat=Xtrainbart, Ydat=Ytrainbart)
  error.folds = rbind(error.folds, tmp)
}
head(error.folds)
```

Finally, we were able to get the test MSE after we iterated through a 5-fold cross validation. 
```{r}
pred.YTest = gbart(Xtrainbart, Ytrainbart, x.test = Xtestbart)
# Test mse
test_mse = mean((Ytestbart - pred.YTest$yhat.test.mean)^2)
test_mse
```

Because the test error was quite high, we decided to check if boosting had a better prediction. To do this, we fit a boosting model. 
```{r}
# Test error was quite high for BART, checking if boosting is any better....
set.seed (1)
boost.music <- gbm(popularity ~ ., data = train,
  distribution = "gaussian", n.trees = 1000,
  interaction.depth = 4)
```

```{r}
yhat.boost <- predict(boost.music ,
  newdata = train, n.trees = 1000)
mean ((yhat.boost - Ytestbart)^2)
```
As we can see [insert the results of the boosting].


## Multivariate Additive Regression Splines (MARS)

The final algorithm we implimented was the multivariate adaptive regression splines (MARS). The MARS model is an ensemble of piecewise linear functions that are joined together by hinge functions. This accounts for non-linearity between inputs and outputs. The MARS algorithm has two stages - a forward stage and a backward stage. The forward stage generates many pairs of functions as candidates for the basis of the model. The backward stage is considered the pruning stage - the model goes through all of the functions and deletes the functons that don't add further value to the model performance. 

We chose to use the MARS algorithm because we can see that the relationship between popularity and our predictors is often non-linear. Additionally, we have many predictor variables and the MARS algorithm is capable of achieving good performance on a regression problem with many predictor variables. 

In a nutshell, the MARS method works like this:
1. Divide a given dataset into k pieces.
2. Fit a regression model to each piece.
3. Use k-fold cross-validation to choose a value for k. 

Our first step was to create a tuning grid for pruning and set a seed for reproductability:
```{r}
library(earth) # library for fitting MARS models
library(caret) # library for tuning the model parameters

trainmars <- train # copy dataset for mars model

#create a tuning grid
hyper_grid <- expand.grid(degree = 1:3,
                          nprune = seq(2, 50, length.out = 10) %>%
                          floor())

#make this example reproducible
set.seed(123)
```

Next, we fit the model with k-fold cross validation:
```{r}
#fit MARS model using k-fold cross-validation
cv_mars <- train(
  x = subset(trainmars, select = -c(popularity)),
  y = train$popularity,
  method = "earth",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = hyper_grid)
```

Finally, we used the MARS algorithm to find the model with the lowest test RMSE:
```{r}
#display model with lowest test RMSE
cv_mars$results %>%
  filter(nprune == cv_mars$bestTune$nprune, degree == cv_mars$bestTune$degree)    
```
We can see that the best model found by the MARS method was of degree 3 with 18 prunes. The R-squared was 0.6928, which is fairly strong correlation for this mode. The test root mean squared error was 8.6215.

Below is a graph of the test RMSE by terms and degree - one can see that the third degree has the lowest RMSE and the number of terms appear to minimize RMSE at a value between 10 and 20 (hence why our value of 18 makes sense).
```{r}
#display test RMSE by terms and degree
ggplot(cv_mars)
```


To compare this model to the other models we fit, we will convert RMSE to MSE:
```{r}
testmse_mars <- cv_mars$results$RMSE[18]^2
testmse_mars
```
Thus, we have a test MSE value of 74.33276 from our model using multivariate adaptive regression splines. 


# Comparing models



# Sources

ISLR (insert page(s)/sections later)

Lab 4 Cross-Validation

![elastic net model information](https://towardsdatascience.com/regulate-your-regression-model-with-ridge-lasso-and-elasticnet-92735e192e34)

![MARS model information](https://towardsdatascience.com/mars-multivariate-adaptive-regression-splines-how-to-improve-on-linear-regression-e1e7a63c5eae)

![MARS code](https://www.statology.org/multivariate-adaptive-regression-splines-in-r/)
